{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CIFAR_Bootstrap_Exp(Num_Tasks,Num_Epochs):\n",
    "    %matplotlib notebook\n",
    "    import cntk\n",
    "    cntk.device.try_set_default_device(cntk.device.gpu(0))\n",
    "    import os\n",
    "    os.environ['KERAS_BACKEND'] = 'cntk'\n",
    "    import keras as keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense,BatchNormalization,Dropout,Flatten, Conv1D, Conv2D, MaxPool2D\n",
    "    from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "    from keras.metrics import categorical_accuracy\n",
    "    from keras import regularizers,optimizers\n",
    "    from keras.regularizers import l2\n",
    "    import numpy as np\n",
    "    import scipy.io\n",
    "    import matplotlib.pyplot as plt\n",
    "    import gzip\n",
    "    from keras.layers.core import Lambda\n",
    "    from scipy.integrate import trapz\n",
    "    import seaborn as sns\n",
    "\n",
    "    from keras.layers.core import Lambda\n",
    "    from keras import backend as K    \n",
    "    \n",
    "    print(K.cntk_backend.dev.use_default_device())\n",
    "\n",
    "    def mini_batches(InputSample,BatchSize):\n",
    "        Index = np.array(range(InputSample.shape[0]),dtype=int)\n",
    "        NumBatches = np.int(InputSample.shape[0]/BatchSize)\n",
    "        Removed = np.array([],dtype=int)\n",
    "\n",
    "        BatchInd =[]\n",
    "        for BatchLoop in range(NumBatches):\n",
    "            RemainIndex = np.delete(Index,Removed)\n",
    "            SampleInd = np.random.choice(RemainIndex,size=BatchSize,replace=False)\n",
    "            Removed = np.append(Removed,SampleInd,axis=0)\n",
    "\n",
    "            BatchInd.append(SampleInd)\n",
    "        RemainIndex = np.delete(Index,Removed)\n",
    "        BatchInd.append(RemainIndex)\n",
    "\n",
    "        return BatchInd,NumBatches\n",
    "\n",
    "    def unpickle(file):\n",
    "        import pickle\n",
    "        with open(file, 'rb') as fo:\n",
    "            dict = pickle.load(fo, encoding='bytes')\n",
    "        return dict\n",
    "\n",
    "    def one_hot(label_list):\n",
    "        NumLabels = len(label_list)\n",
    "        MaxLabel = max(label_list)\n",
    "        one_hot_labels = np.zeros([NumLabels,MaxLabel+1],dtype=int)\n",
    "        for i in range(NumLabels):\n",
    "            one_hot_labels[i,label_list[i]-1] = 1\n",
    "\n",
    "        return one_hot_labels\n",
    "\n",
    "    def one_hot_reverse(one_hot_labels):\n",
    "        labels = []\n",
    "        for i in range(one_hot_labels.shape[0]):\n",
    "            Sample = one_hot_labels[i,:]\n",
    "            labels.append(Sample.argmax())\n",
    "\n",
    "        return np.asarray(labels)\n",
    "\n",
    "    InitStr = 'cifar-10-python\\cifar-10-batches-py\\data_batch_'\n",
    "    File = InitStr + np.str(1)\n",
    "    Batch = unpickle(File)\n",
    "    FullInputs = Batch[b'data']\n",
    "    FullTargets = one_hot(Batch[b'labels'])\n",
    "\n",
    "\n",
    "    for i in  range(4):\n",
    "        File = InitStr + np.str(i+2)\n",
    "        Batch = unpickle(File)\n",
    "        FullInputs = np.append(FullInputs,Batch[b'data'],axis=0)\n",
    "        FullTargets = np.append(FullTargets,one_hot(Batch[b'labels']),axis=0)\n",
    "\n",
    "    Conv = 1\n",
    "    if Conv == 1:\n",
    "        FullInputs = np.reshape(FullInputs,[FullInputs.shape[0],3,32,32]).transpose(0,2,3,1)\n",
    "\n",
    "    FullInputs = FullInputs.copy(order='C')\n",
    "\n",
    "    Validation_Cutoff = 0.75\n",
    "\n",
    "    Validation_Cutoff = np.int(Validation_Cutoff*FullInputs.shape[0])\n",
    "    if Conv == 1:\n",
    "        ValInputs = FullInputs[Validation_Cutoff:,:,:,:]\n",
    "    else:\n",
    "        ValInputs = FullInputs[Validation_Cutoff:,:]\n",
    "    ValTargets = FullTargets[Validation_Cutoff:,:]\n",
    "\n",
    "    if Conv == 1:\n",
    "        TrainInputs = FullInputs[0:Validation_Cutoff,:,:,:]\n",
    "    else:\n",
    "        TrainInputs = FullInputs[0:Validation_Cutoff,:]\n",
    "\n",
    "    TrainTargets = FullTargets[0:Validation_Cutoff,:]\n",
    "\n",
    "\n",
    "    data_dim = TrainInputs.shape[1]\n",
    "    if Conv == 1:\n",
    "        data_dim2 = TrainInputs.shape[2]\n",
    "        NumChannels = 3\n",
    "\n",
    "    NumSamples = TrainInputs.shape[0]\n",
    "    Num_Targets = TrainTargets.shape[-1]\n",
    "\n",
    "    index = np.linspace(0,NumSamples,NumSamples,endpoint=False,dtype=int)\n",
    "\n",
    "    reg_coeff = 0\n",
    "\n",
    "    def Gen_Conv_Model(reg_coeff):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(50,(3,3),activation='relu',input_shape =(data_dim,data_dim2,NumChannels),data_format=\"channels_last\"))\n",
    "        model.add(MaxPool2D())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(50,(3,3),activation='relu',data_format=\"channels_last\"))\n",
    "        model.add(MaxPool2D())\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(100,activation='relu',kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dropout(0.25))\n",
    "    #     model.add(Dense(100,activation='relu',kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "        optim = optimizers.adam(lr=0.00001)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def Gen_FF_Model(reg_coeff):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(300,activation='relu',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(300,activation='relu',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(300,activation='relu',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "        optim = optimizers.adagrad(lr=0.00001,decay=0.95)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "        return model\n",
    "\n",
    "    if Conv == 1:\n",
    "        Uni_model = Gen_Conv_Model(reg_coeff)\n",
    "    else:\n",
    "        Uni_model = Gen_FF_Model(reg_coeff)\n",
    "\n",
    "    Init_Weights = Uni_model.get_weights()\n",
    "\n",
    "    def acquisition_function_BALD(model,samples,Num_Targets,temperature=1,Target_Ratio = 5):\n",
    "        nb_MC_samples = 30\n",
    "        MC_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[-1].output])\n",
    "        MC_samples = np.zeros([nb_MC_samples,samples.shape[0],Num_Targets])\n",
    "        learning_phase = True \n",
    "        for i in range(nb_MC_samples):\n",
    "            MC_samples[i,:,:] = np.array([MC_output([samples, learning_phase])[0]])\n",
    "\n",
    "        expected_entropy = - np.mean(np.sum(MC_samples * np.log(MC_samples + 1e-10), axis=-1), axis=0)  # [batch size]\n",
    "        expected_p = np.mean(MC_samples, axis=0)\n",
    "        entropy_expected_p = - np.sum(expected_p * np.log(expected_p + 1e-10), axis=-1)  # [batch size]\n",
    "        BALD_acq = entropy_expected_p - expected_entropy\n",
    "\n",
    "        Exp_BALD = np.exp(BALD_acq/temperature)\n",
    "        Sampling_Prob = Exp_BALD/np.sum(Exp_BALD).astype(float)\n",
    "\n",
    "        Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "        Target_Ratio = Target_Ratio\n",
    "        if Max_Prob_Ratio < Target_Ratio:\n",
    "            while Max_Prob_Ratio <Target_Ratio:\n",
    "                temperature = temperature*0.99\n",
    "                Exp_BALD = np.exp(BALD_acq/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_BALD/np.sum(Exp_BALD).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        else:\n",
    "            while Max_Prob_Ratio > Target_Ratio:\n",
    "                temperature = temperature*1.01\n",
    "                Exp_BALD = np.exp(BALD_acq/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_BALD/np.sum(Exp_BALD).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        return Sampling_Prob\n",
    "\n",
    "\n",
    "    def acquisition_function_dist_to_threshold(model,samples,Num_Targets,temperature=1,Target_Ratio=5):\n",
    "        Output = model.predict(samples)\n",
    "        Output -= 1/float(Num_Targets)\n",
    "        Dist_to_Threshold = np.sum(np.abs(Output),1)\n",
    "        Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "        Exp_Dist_to_Threshold *= 1\n",
    "    #     Exp_Dist_to_Threshold = Dist_to_Threshold\n",
    "        Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "        Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "        Target_Ratio = Target_Ratio\n",
    "        if Max_Prob_Ratio < Target_Ratio:\n",
    "            while Max_Prob_Ratio <Target_Ratio:\n",
    "                temperature = temperature*0.99\n",
    "                Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        else:\n",
    "            while Max_Prob_Ratio > Target_Ratio:\n",
    "                temperature = temperature*1.01\n",
    "                Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        return Sampling_Prob\n",
    "\n",
    "\n",
    "    count = 0 \n",
    "    Num_Epochs = Num_Epochs\n",
    "    Num_BurnIn = 1\n",
    "    Batch_Size = 100\n",
    "\n",
    "    NumTasks = 3\n",
    "    try:\n",
    "        SwitchPoint = np.int(Num_Epochs/NumTasks)\n",
    "        print(SwitchPoint)\n",
    "    except:\n",
    "        SwitchPoint = 1\n",
    "    Smoothing_Constant = 0\n",
    "\n",
    "    Val_Error = np.zeros([Num_Epochs,4])\n",
    "    Val_Acc = np.zeros([Num_Epochs,4])\n",
    "\n",
    "    Uni_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=Num_Epochs,verbose=1,validation_data=[ValInputs,ValTargets])\n",
    "\n",
    "    TestStr = 'cifar-10-python\\cifar-10-batches-py\\test_batch'\n",
    "\n",
    "    TestInputs = Batch[b'data']\n",
    "    TestTargets = one_hot(Batch[b'labels'])\n",
    "\n",
    "    if Conv == 1:\n",
    "        TestInputs = np.reshape(TestInputs,[TestInputs.shape[0],3,32,32]).transpose(0,2,3,1)\n",
    "\n",
    "    Predicted_Classes = Uni_model.predict_classes(TestInputs)\n",
    "    Test_Labels = one_hot_reverse(TestTargets)\n",
    "    Diff = Predicted_Classes - Test_Labels\n",
    "    Correct = Diff[Diff ==0 ]\n",
    "    Uni_TestError = Correct.shape[0]/Test_Labels.shape[0]\n",
    "\n",
    "    Dist_to_Threshold = acquisition_function_dist_to_threshold(Uni_model,TrainInputs,Num_Targets,1,10)\n",
    "\n",
    "    Easy_Dist_to_Threshold_ArgSort = np.flipud(Dist_to_Threshold.argsort())\n",
    "    Hard_Dist_to_Threshold_ArgSort = Dist_to_Threshold.argsort()\n",
    "\n",
    "    D2T_Easy_Model = Gen_Conv_Model(reg_coeff)\n",
    "    D2T_Easy_Model.set_weights(Init_Weights)\n",
    "    del Uni_model\n",
    "\n",
    "    NumTasks = Num_Tasks\n",
    "    TaskSplitPoints = np.int(np.ceil(NumSamples/NumTasks))\n",
    "    print(TaskSplitPoints)\n",
    "    for i in range(NumTasks):\n",
    "        Num_Epochs_Task = Num_Epochs*(1/(i+1))\n",
    "        TaskInd = Easy_Dist_to_Threshold_ArgSort[0:np.min([(i+1)*TaskSplitPoints,NumSamples])]\n",
    "        D2T_Easy_Model.fit(TrainInputs[TaskInd,:],TrainTargets[TaskInd],batch_size=Batch_Size,epochs = np.int(Num_Epochs_Task))\n",
    "\n",
    "    Predicted_Classes = D2T_Easy_Model.predict_classes(TestInputs)\n",
    "    Test_Labels = one_hot_reverse(TestTargets)\n",
    "    Diff = Predicted_Classes - Test_Labels\n",
    "    Correct = Diff[Diff ==0 ]\n",
    "    D2T_Easy_TestError = Correct.shape[0]/Test_Labels.shape[0]\n",
    "\n",
    "    D2T_Hard_Model = Gen_Conv_Model(reg_coeff)\n",
    "    D2T_Hard_Model.set_weights(Init_Weights)\n",
    "    del D2T_Easy_Model\n",
    "\n",
    "    NumTasks = Num_Tasks\n",
    "    TaskSplitPoints = np.int(np.ceil(NumSamples/NumTasks))\n",
    "    print(TaskSplitPoints)\n",
    "    for i in range(NumTasks):\n",
    "        Num_Epochs_Task = Num_Epochs*(1/(i+1))\n",
    "        TaskInd = Hard_Dist_to_Threshold_ArgSort[0:np.min([(i+1)*TaskSplitPoints,NumSamples])]\n",
    "        D2T_Hard_Model.fit(TrainInputs[TaskInd,:],TrainTargets[TaskInd],batch_size=Batch_Size,epochs = np.int(Num_Epochs_Task))\n",
    "\n",
    "\n",
    "    Predicted_Classes = D2T_Hard_Model.predict_classes(TestInputs)\n",
    "    Test_Labels = one_hot_reverse(TestTargets)\n",
    "    Diff = Predicted_Classes - Test_Labels\n",
    "    Correct = Diff[Diff ==0 ]\n",
    "    D2T_Hard_TestError = Correct.shape[0]/Test_Labels.shape[0]\n",
    "\n",
    "    del D2T_Hard_Model\n",
    "    \n",
    "    Errors = np.array([Uni_TestError,D2T_Hard_TestError,D2T_Easy_TestError])\n",
    "\n",
    "    return Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using CNTK backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU[0] Tesla K40c\n",
      "16\n",
      "Train on 37500 samples, validate on 12500 samples\n",
      "Epoch 1/50\n",
      "37500/37500 [==============================] - 18s 487us/step - loss: 2.8826 - categorical_accuracy: 0.1438 - val_loss: 2.1963 - val_categorical_accuracy: 0.2381\n",
      "Epoch 2/50\n",
      "37500/37500 [==============================] - 18s 467us/step - loss: 2.4661 - categorical_accuracy: 0.2197 - val_loss: 2.0293 - val_categorical_accuracy: 0.2953\n",
      "Epoch 3/50\n",
      "37500/37500 [==============================] - 18s 468us/step - loss: 2.3036 - categorical_accuracy: 0.2556 - val_loss: 1.9222 - val_categorical_accuracy: 0.3316\n",
      "Epoch 4/50\n",
      "37500/37500 [==============================] - 18s 468us/step - loss: 2.1802 - categorical_accuracy: 0.2845 - val_loss: 1.8526 - val_categorical_accuracy: 0.3538\n",
      "Epoch 5/50\n",
      "37500/37500 [==============================] - 18s 467us/step - loss: 2.1067 - categorical_accuracy: 0.3038 - val_loss: 1.7935 - val_categorical_accuracy: 0.3716\n",
      "Epoch 6/50\n",
      "37500/37500 [==============================] - 18s 467us/step - loss: 2.0343 - categorical_accuracy: 0.3210 - val_loss: 1.7477 - val_categorical_accuracy: 0.3872\n",
      "Epoch 7/50\n",
      "37500/37500 [==============================] - 18s 469us/step - loss: 1.9819 - categorical_accuracy: 0.3328 - val_loss: 1.7040 - val_categorical_accuracy: 0.4024\n",
      "Epoch 8/50\n",
      "37500/37500 [==============================] - 18s 470us/step - loss: 1.9241 - categorical_accuracy: 0.3491 - val_loss: 1.6668 - val_categorical_accuracy: 0.4146\n",
      "Epoch 9/50\n",
      "37500/37500 [==============================] - 18s 469us/step - loss: 1.8892 - categorical_accuracy: 0.3589 - val_loss: 1.6403 - val_categorical_accuracy: 0.4236\n",
      "Epoch 10/50\n",
      "37500/37500 [==============================] - 18s 469us/step - loss: 1.8483 - categorical_accuracy: 0.3708 - val_loss: 1.6114 - val_categorical_accuracy: 0.4370\n",
      "Epoch 11/50\n",
      "37500/37500 [==============================] - 18s 468us/step - loss: 1.8172 - categorical_accuracy: 0.3791 - val_loss: 1.5852 - val_categorical_accuracy: 0.4435\n",
      "Epoch 12/50\n",
      "37500/37500 [==============================] - 18s 469us/step - loss: 1.7862 - categorical_accuracy: 0.3868 - val_loss: 1.5601 - val_categorical_accuracy: 0.4526\n",
      "Epoch 13/50\n",
      "37500/37500 [==============================] - 18s 469us/step - loss: 1.7449 - categorical_accuracy: 0.3967 - val_loss: 1.5394 - val_categorical_accuracy: 0.4592\n",
      "Epoch 14/50\n",
      "37500/37500 [==============================] - 18s 468us/step - loss: 1.7215 - categorical_accuracy: 0.4037 - val_loss: 1.5152 - val_categorical_accuracy: 0.4662\n",
      "Epoch 15/50\n",
      "37500/37500 [==============================] - 18s 468us/step - loss: 1.7018 - categorical_accuracy: 0.4100 - val_loss: 1.4972 - val_categorical_accuracy: 0.4728\n",
      "Epoch 16/50\n",
      "37500/37500 [==============================] - 18s 468us/step - loss: 1.6703 - categorical_accuracy: 0.4185 - val_loss: 1.4811 - val_categorical_accuracy: 0.4781\n",
      "Epoch 17/50\n",
      "37500/37500 [==============================] - 18s 468us/step - loss: 1.6477 - categorical_accuracy: 0.4272 - val_loss: 1.4615 - val_categorical_accuracy: 0.4842\n",
      "Epoch 18/50\n",
      "37500/37500 [==============================] - 18s 468us/step - loss: 1.6267 - categorical_accuracy: 0.4323 - val_loss: 1.4517 - val_categorical_accuracy: 0.4877\n",
      "Epoch 19/50\n",
      "37500/37500 [==============================] - 18s 468us/step - loss: 1.6055 - categorical_accuracy: 0.4396 - val_loss: 1.4345 - val_categorical_accuracy: 0.4946\n",
      "Epoch 20/50\n",
      "37500/37500 [==============================] - 18s 468us/step - loss: 1.5817 - categorical_accuracy: 0.4463 - val_loss: 1.4197 - val_categorical_accuracy: 0.4983\n",
      "Epoch 21/50\n",
      "37500/37500 [==============================] - 18s 470us/step - loss: 1.5711 - categorical_accuracy: 0.4454 - val_loss: 1.4060 - val_categorical_accuracy: 0.5047\n",
      "Epoch 22/50\n",
      "37500/37500 [==============================] - 18s 467us/step - loss: 1.5542 - categorical_accuracy: 0.4560 - val_loss: 1.3925 - val_categorical_accuracy: 0.5078\n",
      "Epoch 23/50\n",
      "37500/37500 [==============================] - 18s 468us/step - loss: 1.5325 - categorical_accuracy: 0.4627 - val_loss: 1.3832 - val_categorical_accuracy: 0.5121\n",
      "Epoch 24/50\n",
      "37500/37500 [==============================] - 18s 468us/step - loss: 1.5180 - categorical_accuracy: 0.4653 - val_loss: 1.3731 - val_categorical_accuracy: 0.5147\n",
      "Epoch 25/50\n",
      "37500/37500 [==============================] - 18s 473us/step - loss: 1.5100 - categorical_accuracy: 0.4698 - val_loss: 1.3621 - val_categorical_accuracy: 0.5197\n",
      "Epoch 26/50\n",
      " 2100/37500 [>.............................] - ETA: 15s - loss: 1.5120 - categorical_accuracy: 0.4676"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "NumTests = 100\n",
    "from IPython.display import clear_output\n",
    "\n",
    "TestError_Record = np.zeros([3,NumTests])\n",
    "\n",
    "for i in range(NumTests):\n",
    "    clear_output()\n",
    "    print('Run:')\n",
    "    print(i)\n",
    "    TestError_Record[:,i] = CIFAR_Bootstrap_Exp(5,50)\n",
    "    np.save('CIFAR_Bootstrap_Perf_5_lr0.00001',TestError_Record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
