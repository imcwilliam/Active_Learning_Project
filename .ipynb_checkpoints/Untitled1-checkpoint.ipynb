{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def Run_MNIST_Exp(Num_Epochs):\n",
    "    %matplotlib notebook\n",
    "    import keras as keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense,BatchNormalization,Dropout,Flatten, Conv1D\n",
    "    from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "    from keras.metrics import categorical_accuracy\n",
    "    from keras import regularizers,optimizers\n",
    "    from keras.regularizers import l2\n",
    "    import numpy as np\n",
    "    import scipy.io\n",
    "    import matplotlib.pyplot as plt\n",
    "    import gzip\n",
    "    from keras.layers.core import Lambda\n",
    "    from scipy.integrate import trapz\n",
    "    import seaborn as sns\n",
    "\n",
    "    from keras.layers.core import Lambda\n",
    "    from keras import backend as K    \n",
    "\n",
    "\n",
    "    def mini_batches(InputSample,BatchSize):\n",
    "        Index = np.array(range(InputSample.shape[0]),dtype=int)\n",
    "        NumBatches = np.int(InputSample.shape[0]/BatchSize)\n",
    "        Removed = np.array([],dtype=int)\n",
    "\n",
    "        BatchInd =[]\n",
    "        for BatchLoop in range(NumBatches):\n",
    "            RemainIndex = np.delete(Index,Removed)\n",
    "            SampleInd = np.random.choice(RemainIndex,size=BatchSize,replace=False)\n",
    "            Removed = np.append(Removed,SampleInd,axis=0)\n",
    "\n",
    "            BatchInd.append(SampleInd)\n",
    "        RemainIndex = np.delete(Index,Removed)\n",
    "        BatchInd.append(RemainIndex)\n",
    "\n",
    "        return BatchInd,NumBatches\n",
    "\n",
    "    def Get_Feats_and_Targets(filename):\n",
    "        import numpy as np\n",
    "\n",
    "        def line_to_Feats(line):\n",
    "            line = line.split(' ')\n",
    "            Feats = np.asarray(line[0:1024])\n",
    "            Target = np.zeros([3])\n",
    "            Target[int(line[1024])] = 1\n",
    "            return Feats,Target\n",
    "\n",
    "        f = open(filename, 'r')\n",
    "        lines = f.readlines()\n",
    "        Features = []\n",
    "        Targets = []\n",
    "        for i in range(len(lines)-1):\n",
    "            line = lines[i+1]\n",
    "            Feats,Tgts = line_to_Feats(line)\n",
    "            Features.append(Feats)\n",
    "            Targets.append(Tgts)\n",
    "\n",
    "        return np.asarray(Features,dtype = 'float64'), np.asarray(Targets,dtype = 'int')\n",
    "\n",
    "\n",
    "    FullInputs = scipy.io.loadmat('MNIST_TrainInputs.mat')\n",
    "    FullInputs = FullInputs['images']\n",
    "\n",
    "    FullTargets = scipy.io.loadmat('MNIST_TrainTargets.mat')\n",
    "    FullTargets = FullTargets['targets']\n",
    "\n",
    "    Validation_Cutoff = 0.5\n",
    "\n",
    "    Validation_Cutoff = np.int(Validation_Cutoff*FullInputs.shape[0])\n",
    "\n",
    "    ValInputs = FullInputs[Validation_Cutoff:,:]\n",
    "    ValTargets = FullTargets[Validation_Cutoff:,:]\n",
    "\n",
    "    TrainInputs = FullInputs[0:Validation_Cutoff,:]\n",
    "    TrainTargets = FullTargets[0:Validation_Cutoff,:]\n",
    "\n",
    "    data_dim = TrainInputs.shape[-1]\n",
    "    NumSamples = TrainInputs.shape[0]\n",
    "    Num_Targets = TrainTargets.shape[-1]\n",
    "\n",
    "    index = np.linspace(0,NumSamples,NumSamples,endpoint=False,dtype=int)\n",
    "\n",
    "    reg_coeff = 0.001\n",
    "\n",
    "    def Gen_Model(reg_coeff):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "        optim = optimizers.sgd(lr=0.001)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "        return model\n",
    "    \n",
    "    \n",
    "#     def Gen_Model(reg_coeff):\n",
    "#         model = Sequential()\n",
    "#         model.add(Conv2D(100,5,activation='relu',input_shape =(data_dim,data_dim2,1)))\n",
    "#         model.add(MaxPool2D())\n",
    "#         model.add(Conv2D(100,5,activation='relu'))\n",
    "#         model.add(MaxPool2D())\n",
    "#         model.add(Flatten())\n",
    "#         model.add(Dropout(0.5))\n",
    "#         model.add(Dense(100,activation='relu',kernel_regularizer=l2(reg_coeff)))\n",
    "#         model.add(Dropout(0.5))\n",
    "#         model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "#         optim = optimizers.sgd(lr=0.001)\n",
    "#         model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "\n",
    "#         return model\n",
    "\n",
    "    ISHard_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    ISEasy_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    Uni_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    ISHard_model.set_weights(Uni_model.get_weights())\n",
    "    ISEasy_model.set_weights(Uni_model.get_weights())\n",
    "\n",
    "    def average_dist_to_threshold(model,samples,Num_Targets):\n",
    "        Output = model.predict(samples)\n",
    "        Output -= 1/float(Num_Targets)\n",
    "        Dist_to_Threshold = np.mean(np.abs(Output),1)\n",
    "        return Dist_to_Threshold\n",
    "\n",
    "    def acquisition_function_dist_to_threshold(model,samples,Num_Targets,temperature=1,Rescale=0):\n",
    "        Output = model.predict(samples)\n",
    "        Output -= 1/float(Num_Targets)\n",
    "        Dist_to_Threshold = np.sum(np.abs(Output),1)\n",
    "        Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "        Exp_Dist_to_Threshold *= 1\n",
    "    #     Exp_Dist_to_Threshold = Dist_to_Threshold\n",
    "        Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "        if Rescale == 1:\n",
    "            Median_Prob = np.median(Sampling_Prob)\n",
    "            Min_Prob = np.min(Sampling_Prob)\n",
    "            Sampling_Prob[Sampling_Prob > Median_Prob] += Min_Prob/2\n",
    "            Sampling_Prob[Sampling_Prob < Median_Prob] -= Min_Prob/2\n",
    "        return Sampling_Prob\n",
    "\n",
    "    count = 0 \n",
    "    Num_Epochs = Num_Epochs\n",
    "    Num_BurnIn = 1\n",
    "    Batch_Size = 30\n",
    "\n",
    "    Smoothing_Constant = 0\n",
    "\n",
    "    Val_Error = np.zeros([Num_Epochs,3])\n",
    "    Val_Acc = np.zeros([Num_Epochs,3])\n",
    "\n",
    "    def Biased_Batch(Inputs,Targets,Batch_Size,Sampling_Probability,Weighting_Flag = 1):\n",
    "        while True: \n",
    "            index = np.linspace(0,Inputs.shape[0],Inputs.shape[0],endpoint=False,dtype=int)\n",
    "    #         Sampling_Probability = Sampling_Probability/Sampling_Probability.sum()\n",
    "            Batch = np.random.choice(index,size=Batch_Size,replace=False,p=Sampling_Probability)\n",
    "            Batch_Inputs = Inputs[Batch,:]\n",
    "            Batch_Targets = Targets[Batch,:]\n",
    "            if Weighting_Flag == 1:\n",
    "                FullWeights = Sampling_Probability**-1\n",
    "                FullWeights = FullWeights/np.mean(FullWeights)\n",
    "            elif Weighting_Flag == -1:\n",
    "                FullWeights = Sampling_Probability/np.mean(Sampling_Probability)\n",
    "            else:\n",
    "                FullWeights = np.ones(Inputs.shape[0])\n",
    "\n",
    "            Weights = FullWeights[Batch]\n",
    "            yield (Batch_Inputs,Batch_Targets,Weights)\n",
    "\n",
    "\n",
    "    for EpochLoop in  range(Num_Epochs):\n",
    "        if count < Num_BurnIn:\n",
    "    #         ISHard_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "    #         Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "    #         Val_Error[count,0] = Error[0]\n",
    "    #         Val_Acc[count,0] = Error[1]\n",
    "\n",
    "    #         ISEasy_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,verbose=0)\n",
    "    #         Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "    #         Val_Error[count,1] = Error[0]\n",
    "    #         Val_Acc[count,1] = Error[1]\n",
    "\n",
    "            Uni_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,verbose=0)\n",
    "            Error = Uni_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,2] = Error[0]\n",
    "            Val_Acc[count,2] = Error[1]\n",
    "\n",
    "            ISHard_model.set_weights(Uni_model.get_weights())\n",
    "            ISEasy_model.set_weights(Uni_model.get_weights())\n",
    "\n",
    "            Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,0] = Error[0]\n",
    "            Val_Acc[count,0] = Error[1]\n",
    "\n",
    "            Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,1] = Error[0]\n",
    "            Val_Acc[count,1] = Error[1]\n",
    "\n",
    "\n",
    "        else:\n",
    "            Hard_Sampling_Probability = acquisition_function_dist_to_threshold(ISHard_model,TrainInputs,Num_Targets,0.6)\n",
    "            Easy_SamplingProbability = 1/acquisition_function_dist_to_threshold(ISEasy_model,TrainInputs,Num_Targets,0.6)\n",
    "            Easy_SamplingProbability/= Easy_SamplingProbability.sum().astype(float)\n",
    "#             print(Hard_Sampling_Probability.max()/Hard_Sampling_Probability.min())\n",
    "\n",
    "            MaxProbInd = Hard_Sampling_Probability.argmax()\n",
    "            MinProbInd = Hard_Sampling_Probability.argmin()\n",
    "\n",
    "            ISHard_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,Hard_Sampling_Probability,0),\n",
    "                                       steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "            ISEasy_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,Easy_SamplingProbability,0),\n",
    "                                     steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "\n",
    "            Uni_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,1/np.float(NumSamples)*np.ones(NumSamples)),\n",
    "                                    steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "            Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,0] = Error[0]\n",
    "            Val_Acc[count,0] = Error[1]\n",
    "\n",
    "            Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,1] = Error[0]\n",
    "            Val_Acc[count,1] = Error[1]\n",
    "\n",
    "            Error = Uni_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,2] = Error[0]\n",
    "            Val_Acc[count,2] = Error[1]\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    TestInputs = scipy.io.loadmat('MNIST_TestInputs.mat')\n",
    "    TestInputs = TestInputs['test_images']\n",
    "    TestTargets = scipy.io.loadmat('MNIST_TestTargets.mat')\n",
    "    TestTargets = TestTargets['test_targets']\n",
    "\n",
    "    TestError = np.zeros([3,2])\n",
    "    TestError[0,:] = ISHard_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[1,:] = ISEasy_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[2,:] = Uni_model.test_on_batch(TestInputs,TestTargets)\n",
    "\n",
    "    return TestError, Val_Error,Val_Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "NumRuns = 1000\n",
    "NumEpochs = 256\n",
    "\n",
    "TestError = np.zeros([3,2,NumRuns])\n",
    "Val_Error = np.zeros([NumEpochs,3,NumRuns])\n",
    "Val_Acc = np.zeros([NumEpochs,3,NumRuns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-532500e776f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Val_Error_MNIST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVal_Error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Val_Acc_MNIST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVal_Acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mTestError\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVal_Error\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mVal_Acc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRun_MNIST_Exp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNumEpochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2ecc7f285c6c>\u001b[0m in \u001b[0;36mRun_MNIST_Exp\u001b[0;34m(Num_Epochs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mHard_Sampling_Probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macquisition_function_dist_to_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mISHard_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrainInputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNum_Targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0mEasy_SamplingProbability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0macquisition_function_dist_to_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mISEasy_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTrainInputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNum_Targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0mEasy_SamplingProbability\u001b[0m\u001b[0;34m/=\u001b[0m \u001b[0mEasy_SamplingProbability\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2ecc7f285c6c>\u001b[0m in \u001b[0;36macquisition_function_dist_to_threshold\u001b[0;34m(model, samples, Num_Targets, temperature, Rescale)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0macquisition_function_dist_to_threshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mNum_Targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRescale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0mOutput\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNum_Targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mDist_to_Threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ianmcwilliam/anaconda/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m         return self.model.predict(x, batch_size=batch_size, verbose=verbose,\n\u001b[0;32m-> 1025\u001b[0;31m                                   steps=steps)\n\u001b[0m\u001b[1;32m   1026\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ianmcwilliam/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1833\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1834\u001b[0m         return self._predict_loop(f, ins, batch_size=batch_size,\n\u001b[0;32m-> 1835\u001b[0;31m                                   verbose=verbose, steps=steps)\n\u001b[0m\u001b[1;32m   1836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1837\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[0;32m/Users/ianmcwilliam/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_predict_loop\u001b[0;34m(self, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1337\u001b[0m                         \u001b[0mouts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_out\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1339\u001b[0;31m                     \u001b[0mouts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1340\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1341\u001b[0m                     \u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(NumRuns):\n",
    "    np.save(\"Test_Performance_MNIST\",TestError)\n",
    "    np.save(\"Val_Error_MNIST\",Val_Error)\n",
    "    np.save(\"Val_Acc_MNIST\",Val_Acc)\n",
    "    TestError[:,:,i],Val_Error[:,:,i],Val_Acc[:,:,i] = Run_MNIST_Exp(NumEpochs)\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.78661132, 1.37352991])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Val_Error[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Val_Error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
