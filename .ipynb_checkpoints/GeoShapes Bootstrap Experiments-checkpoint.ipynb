{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def Get_Feats_and_Targets(filename):\n",
    "    import numpy as np\n",
    "\n",
    "    def line_to_Feats(line):\n",
    "        line = line.split(' ')\n",
    "        Feats = np.asarray(line[0:1024])\n",
    "        Target = np.zeros([3])\n",
    "        Target[int(line[1024])] = 1\n",
    "        return Feats,Target\n",
    "\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    Features = []\n",
    "    Targets = []\n",
    "    for i in range(len(lines)-1):\n",
    "        line = lines[i+1]\n",
    "        Feats,Tgts = line_to_Feats(line)\n",
    "        Features.append(Feats)\n",
    "        Targets.append(Tgts)\n",
    "\n",
    "    return np.asarray(Features,dtype = 'float64'), np.asarray(Targets,dtype = 'int')\n",
    "\n",
    "[TrainInputs_Easy,TrainTargets_Easy] = Get_Feats_and_Targets('shapeset1_1cspo_2_3.10000.train.amat')\n",
    "# [ValInputs_Easy,ValTargets_Easy] = Get_Feats_and_Targets('shapeset1_1cspo_2_3.5000.valid.amat')\n",
    "\n",
    "[TrainInputs,TrainTargets] = Get_Feats_and_Targets('shapeset2_1cspo_2_3.10000.train.amat')\n",
    "[ValInputs,ValTargets] = Get_Feats_and_Targets('shapeset2_1cspo_2_3.5000.valid.amat')\n",
    "\n",
    "TrainInputs = np.append(TrainInputs,TrainInputs_Easy,axis=0)\n",
    "TrainTargets = np.append(TrainTargets,TrainTargets_Easy,axis=0)\n",
    "\n",
    "[TestInputs,TestTargets] = Get_Feats_and_Targets('shapeset2_1cspo_2_3.5000.test.amat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GeoShapes_Bootstrap_Exp(TrainInputs,TrainTargets,ValInputs,ValTargets,TestInputs,TestTargets,NumTasks,Num_Epochs = 256):\n",
    "    %matplotlib notebook\n",
    "    import keras as keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense,BatchNormalization,Dropout,Flatten, Conv1D\n",
    "    from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "    from keras.metrics import categorical_accuracy\n",
    "    from keras import regularizers,optimizers\n",
    "    from keras.regularizers import l2\n",
    "    import numpy as np\n",
    "    import scipy.io\n",
    "    import matplotlib.pyplot as plt\n",
    "    import gzip\n",
    "    from keras.layers.core import Lambda\n",
    "    from scipy.integrate import trapz\n",
    "    import seaborn as sns\n",
    "\n",
    "    from keras.layers.core import Lambda\n",
    "    from keras import backend as K    \n",
    "\n",
    "    data_dim = TrainInputs.shape[-1]\n",
    "    NumSamples = TrainInputs.shape[0]\n",
    "    Num_Targets = TrainTargets.shape[-1]\n",
    "\n",
    "    index = np.linspace(0,NumSamples,NumSamples,endpoint=False,dtype=int)\n",
    "\n",
    "    reg_coeff = 0\n",
    "\n",
    "    def Gen_Model(reg_coeff):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(300,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(300,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(300,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "        optim = optimizers.adam(lr=0.0001)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "        return model\n",
    "\n",
    "    D2THard_model = Gen_Model(reg_coeff)\n",
    "    D2TEasy_model = Gen_Model(reg_coeff)\n",
    "    BALDHard_model = Gen_Model(reg_coeff)\n",
    "    BALDEasy_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    Uni_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    D2THard_model.set_weights(Uni_model.get_weights())\n",
    "    D2TEasy_model.set_weights(Uni_model.get_weights())\n",
    "    BALDHard_model.set_weights(Uni_model.get_weights())\n",
    "    BALDEasy_model.set_weights(Uni_model.get_weights())\n",
    "\n",
    "    def acquisition_function_BALD(model,samples,Num_Targets,temperature=1,Target_Ratio = 5):\n",
    "        nb_MC_samples = 30\n",
    "        MC_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[-1].output])\n",
    "        MC_samples = np.zeros([nb_MC_samples,samples.shape[0],Num_Targets])\n",
    "        learning_phase = True \n",
    "        for i in range(nb_MC_samples):\n",
    "            MC_samples[i,:,:] = np.array([MC_output([samples, learning_phase])[0]])\n",
    "\n",
    "        expected_entropy = - np.mean(np.sum(MC_samples * np.log(MC_samples + 1e-10), axis=-1), axis=0)  # [batch size]\n",
    "        expected_p = np.mean(MC_samples, axis=0)\n",
    "        entropy_expected_p = - np.sum(expected_p * np.log(expected_p + 1e-10), axis=-1)  # [batch size]\n",
    "        BALD_acq = entropy_expected_p - expected_entropy\n",
    "\n",
    "        Exp_BALD = np.exp(BALD_acq/temperature)\n",
    "        Sampling_Prob = Exp_BALD/np.sum(Exp_BALD).astype(float)\n",
    "\n",
    "        Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "        Target_Ratio = Target_Ratio\n",
    "        if Max_Prob_Ratio < Target_Ratio:\n",
    "            while Max_Prob_Ratio <Target_Ratio:\n",
    "                temperature = temperature*0.99\n",
    "                Exp_BALD = np.exp(BALD_acq/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_BALD/np.sum(Exp_BALD).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        else:\n",
    "            while Max_Prob_Ratio > Target_Ratio:\n",
    "                temperature = temperature*1.01\n",
    "                Exp_BALD = np.exp(BALD_acq/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_BALD/np.sum(Exp_BALD).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        return Sampling_Prob\n",
    "\n",
    "\n",
    "    def acquisition_function_dist_to_threshold(model,samples,Num_Targets,temperature=1,Target_Ratio=5):\n",
    "        Output = model.predict(samples)\n",
    "        Output -= 1/float(Num_Targets)\n",
    "        Dist_to_Threshold = np.sum(np.abs(Output),1)\n",
    "        Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "        Exp_Dist_to_Threshold *= 1\n",
    "    #     Exp_Dist_to_Threshold = Dist_to_Threshold\n",
    "        Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "        Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "        Target_Ratio = Target_Ratio\n",
    "        if Max_Prob_Ratio < Target_Ratio:\n",
    "            while Max_Prob_Ratio <Target_Ratio:\n",
    "                temperature = temperature*0.99\n",
    "                Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        else:\n",
    "            while Max_Prob_Ratio > Target_Ratio:\n",
    "                temperature = temperature*1.01\n",
    "                Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        return Sampling_Prob\n",
    "\n",
    "    def acquisition_function_entropy(model,samples,Num_Targets,temperature=1,Target_Ratio=5):\n",
    "        Output = model.predict(samples)\n",
    "        Entropy = -np.sum(Output * np.log(Output),1)\n",
    "        Exp_Entropy = np.exp(Entropy/temperature)\n",
    "        Sampling_Prob = Exp_Entropy/np.sum(Exp_Entropy).astype(float)\n",
    "        Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "        Target_Ratio = Target_Ratio\n",
    "        if Max_Prob_Ratio < Target_Ratio:\n",
    "            while Max_Prob_Ratio <Target_Ratio:\n",
    "                temperature = temperature*0.99\n",
    "                Exp_Entropy = np.exp(Entropy/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_Entropy/np.sum(Exp_Entropy).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        else:\n",
    "            while Max_Prob_Ratio > Target_Ratio:\n",
    "                temperature = temperature*1.01\n",
    "                Exp_Entropy = np.exp(Entropy/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_Entropy/np.sum(Exp_Entropy).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        return Sampling_Prob\n",
    "\n",
    "    count = 0 \n",
    "    Num_Epochs = Num_Epochs\n",
    "    Num_BurnIn = 1\n",
    "    Batch_Size = 32\n",
    "\n",
    "    Val_Error = np.zeros([Num_Epochs,4])\n",
    "    Val_Acc = np.zeros([Num_Epochs,4])\n",
    "\n",
    "    Uni_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=Num_Epochs,verbose=1)\n",
    "\n",
    "    Dist_to_Threshold = acquisition_function_dist_to_threshold(Uni_model,TrainInputs,Num_Targets,1,10)\n",
    "    BALD = acquisition_function_BALD(Uni_model,TrainInputs,Num_Targets,1,10)\n",
    "\n",
    "    #'Easiest' First\n",
    "    Easy_Dist_to_Threshold_ArgSort = np.flipud(Dist_to_Threshold.argsort())\n",
    "    Hard_Dist_to_Threshold_ArgSort = Dist_to_Threshold.argsort()\n",
    "    Low_BALD_ArgSort = BALD.argsort()\n",
    "    High_BALD_ArgSort = np.flipud(BALD.argsort())\n",
    "\n",
    "    NumTasks = NumTasks\n",
    "    TaskSplitPoints = np.int(NumSamples/NumTasks)\n",
    "    print(TaskSplitPoints)\n",
    "    for i in range(NumTasks):\n",
    "        Num_Epochs_Task = Num_Epochs*(1/(i+1))\n",
    "\n",
    "        TaskInd = Hard_Dist_to_Threshold_ArgSort[0:(i+1)*TaskSplitPoints]\n",
    "        D2THard_model.fit(TrainInputs[TaskInd,:],TrainTargets[TaskInd],batch_size=Batch_Size,epochs = np.int(Num_Epochs_Task))\n",
    "\n",
    "        TaskInd = Easy_Dist_to_Threshold_ArgSort[0:(i+1)*TaskSplitPoints]\n",
    "        D2TEasy_model.fit(TrainInputs[TaskInd,:],TrainTargets[TaskInd],batch_size=Batch_Size,epochs = np.int(Num_Epochs_Task))\n",
    "\n",
    "#         TaskInd = Low_BALD_ArgSort[0:(i+1)*TaskSplitPoints]\n",
    "#         BALDEasy_model.fit(TrainInputs[TaskInd,:],TrainTargets[TaskInd],batch_size=Batch_Size,epochs = np.int(Num_Epochs_Task))\n",
    "\n",
    "#         TaskInd = High_BALD_ArgSort[0:(i+1)*TaskSplitPoints]\n",
    "#         BALDHard_model.fit(TrainInputs[TaskInd,:],TrainTargets[TaskInd],batch_size=Batch_Size,epochs = np.int(Num_Epochs_Task))\n",
    "\n",
    "    \n",
    "\n",
    "    TestError = np.zeros([5,2])\n",
    "    TestError[0,:] = D2THard_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[1,:] = D2TEasy_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[2,:] = BALDHard_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[3,:] = BALDEasy_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[4,:] = Uni_model.test_on_batch(TestInputs,TestTargets)\n",
    "    \n",
    "    \n",
    "    return TestError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NumTests = 100\n",
    "\n",
    "NumTasks = np.array([2])\n",
    "\n",
    "TestError_Record = np.zeros([5,2,NumTests,NumTasks.shape[0]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:\n",
      "2\n",
      "NumTasks:\n",
      "2\n",
      "Epoch 1/256\n",
      "20000/20000 [==============================] - 5s 257us/step - loss: 1.1681 - categorical_accuracy: 0.3417\n",
      "Epoch 2/256\n",
      "20000/20000 [==============================] - 4s 216us/step - loss: 1.1403 - categorical_accuracy: 0.3487\n",
      "Epoch 3/256\n",
      "20000/20000 [==============================] - 4s 216us/step - loss: 1.1313 - categorical_accuracy: 0.3562\n",
      "Epoch 4/256\n",
      "20000/20000 [==============================] - 4s 216us/step - loss: 1.1207 - categorical_accuracy: 0.3681\n",
      "Epoch 5/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 1.1193 - categorical_accuracy: 0.3639\n",
      "Epoch 6/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 1.1068 - categorical_accuracy: 0.3786\n",
      "Epoch 7/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 1.1004 - categorical_accuracy: 0.3945\n",
      "Epoch 8/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 1.0912 - categorical_accuracy: 0.4018\n",
      "Epoch 9/256\n",
      "20000/20000 [==============================] - 4s 218us/step - loss: 1.0844 - categorical_accuracy: 0.4101\n",
      "Epoch 10/256\n",
      "20000/20000 [==============================] - 4s 216us/step - loss: 1.0687 - categorical_accuracy: 0.4311\n",
      "Epoch 11/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 1.0562 - categorical_accuracy: 0.4455\n",
      "Epoch 12/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 1.0418 - categorical_accuracy: 0.4636\n",
      "Epoch 13/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 1.0285 - categorical_accuracy: 0.4789\n",
      "Epoch 14/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 1.0132 - categorical_accuracy: 0.4977\n",
      "Epoch 15/256\n",
      "20000/20000 [==============================] - 4s 216us/step - loss: 1.0011 - categorical_accuracy: 0.5091\n",
      "Epoch 16/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.9865 - categorical_accuracy: 0.5205\n",
      "Epoch 17/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.9771 - categorical_accuracy: 0.5282\n",
      "Epoch 18/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.9688 - categorical_accuracy: 0.5361\n",
      "Epoch 19/256\n",
      "20000/20000 [==============================] - 4s 216us/step - loss: 0.9522 - categorical_accuracy: 0.5474\n",
      "Epoch 20/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.9453 - categorical_accuracy: 0.5583\n",
      "Epoch 21/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.9346 - categorical_accuracy: 0.5590\n",
      "Epoch 22/256\n",
      "20000/20000 [==============================] - 4s 218us/step - loss: 0.9322 - categorical_accuracy: 0.5589\n",
      "Epoch 23/256\n",
      "20000/20000 [==============================] - 4s 216us/step - loss: 0.9225 - categorical_accuracy: 0.5686\n",
      "Epoch 24/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.9186 - categorical_accuracy: 0.5717\n",
      "Epoch 25/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.9074 - categorical_accuracy: 0.5767\n",
      "Epoch 26/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.9006 - categorical_accuracy: 0.5837\n",
      "Epoch 27/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.8919 - categorical_accuracy: 0.5842\n",
      "Epoch 28/256\n",
      "20000/20000 [==============================] - 4s 216us/step - loss: 0.8871 - categorical_accuracy: 0.5916\n",
      "Epoch 29/256\n",
      "20000/20000 [==============================] - 4s 218us/step - loss: 0.8771 - categorical_accuracy: 0.5957\n",
      "Epoch 30/256\n",
      "20000/20000 [==============================] - 4s 215us/step - loss: 0.8724 - categorical_accuracy: 0.5999\n",
      "Epoch 31/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.8677 - categorical_accuracy: 0.6025\n",
      "Epoch 32/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.8629 - categorical_accuracy: 0.6036\n",
      "Epoch 33/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.8586 - categorical_accuracy: 0.6082\n",
      "Epoch 34/256\n",
      "20000/20000 [==============================] - 4s 216us/step - loss: 0.8484 - categorical_accuracy: 0.6117\n",
      "Epoch 35/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.8476 - categorical_accuracy: 0.6105\n",
      "Epoch 36/256\n",
      "20000/20000 [==============================] - 4s 216us/step - loss: 0.8339 - categorical_accuracy: 0.6191\n",
      "Epoch 37/256\n",
      "20000/20000 [==============================] - 4s 217us/step - loss: 0.8279 - categorical_accuracy: 0.6269\n",
      "Epoch 38/256\n",
      "20000/20000 [==============================] - 4s 216us/step - loss: 0.8213 - categorical_accuracy: 0.6266\n",
      "Epoch 39/256\n",
      "20000/20000 [==============================] - 4s 216us/step - loss: 0.8167 - categorical_accuracy: 0.6322\n",
      "Epoch 40/256\n",
      " 6624/20000 [========>.....................] - ETA: 2s - loss: 0.7987 - categorical_accuracy: 0.6479"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for i in range(NumTests):\n",
    "    for j in range(NumTasks.shape[0]):\n",
    "        clear_output()\n",
    "        print('Run:')\n",
    "        print(i)\n",
    "        print('NumTasks:')\n",
    "        print(NumTasks[j])\n",
    "        TestError_Record[:,:,i,j] = GeoShapes_Bootstrap_Exp(TrainInputs,TrainTargets,\n",
    "                                                            ValInputs,ValTargets,TestInputs,\n",
    "                                                            TestTargets,NumTasks[j],256)\n",
    "        np.save('GeoShapes_Bootstrap_Exp_2',TestError_Record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
