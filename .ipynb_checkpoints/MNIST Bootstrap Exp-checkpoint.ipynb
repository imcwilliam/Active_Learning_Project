{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MNIST_Bootstrap_Exp(Num_Tasks,Num_Epochs):\n",
    "    %matplotlib notebook\n",
    "    import keras as keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense,BatchNormalization,Dropout,Flatten, Conv1D\n",
    "    from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "    from keras.metrics import categorical_accuracy\n",
    "    from keras import regularizers,optimizers\n",
    "    from keras.regularizers import l2\n",
    "    import numpy as np\n",
    "    import scipy.io\n",
    "    import matplotlib.pyplot as plt\n",
    "    import gzip\n",
    "    from keras.layers.core import Lambda\n",
    "    from scipy.integrate import trapz\n",
    "    import seaborn as sns\n",
    "\n",
    "    from keras.layers.core import Lambda\n",
    "    from keras import backend as K    \n",
    "\n",
    "\n",
    "    def mini_batches(InputSample,BatchSize):\n",
    "        Index = np.array(range(InputSample.shape[0]),dtype=int)\n",
    "        NumBatches = np.int(InputSample.shape[0]/BatchSize)\n",
    "        Removed = np.array([],dtype=int)\n",
    "\n",
    "        BatchInd =[]\n",
    "        for BatchLoop in range(NumBatches):\n",
    "            RemainIndex = np.delete(Index,Removed)\n",
    "            SampleInd = np.random.choice(RemainIndex,size=BatchSize,replace=False)\n",
    "            Removed = np.append(Removed,SampleInd,axis=0)\n",
    "\n",
    "            BatchInd.append(SampleInd)\n",
    "        RemainIndex = np.delete(Index,Removed)\n",
    "        BatchInd.append(RemainIndex)\n",
    "\n",
    "        return BatchInd,NumBatches\n",
    "\n",
    "    def Get_Feats_and_Targets(filename):\n",
    "        import numpy as np\n",
    "\n",
    "        def line_to_Feats(line):\n",
    "            line = line.split(' ')\n",
    "            Feats = np.asarray(line[0:1024])\n",
    "            Target = np.zeros([3])\n",
    "            Target[int(line[1024])] = 1\n",
    "            return Feats,Target\n",
    "\n",
    "        f = open(filename, 'r')\n",
    "        lines = f.readlines()\n",
    "        Features = []\n",
    "        Targets = []\n",
    "        for i in range(len(lines)-1):\n",
    "            line = lines[i+1]\n",
    "            Feats,Tgts = line_to_Feats(line)\n",
    "            Features.append(Feats)\n",
    "            Targets.append(Tgts)\n",
    "\n",
    "        return np.asarray(Features,dtype = 'float64'), np.asarray(Targets,dtype = 'int')\n",
    "\n",
    "    FullInputs = scipy.io.loadmat('MNIST_TrainInputs.mat')\n",
    "    FullInputs = FullInputs['images']\n",
    "\n",
    "    FullTargets = scipy.io.loadmat('MNIST_TrainTargets.mat')\n",
    "    FullTargets = FullTargets['targets']\n",
    "\n",
    "    Validation_Cutoff = 0.75\n",
    "\n",
    "    Validation_Cutoff = np.int(Validation_Cutoff*FullInputs.shape[0])\n",
    "\n",
    "    ValInputs = FullInputs[Validation_Cutoff:,:]\n",
    "    ValTargets = FullTargets[Validation_Cutoff:,:]\n",
    "\n",
    "    TrainInputs = FullInputs[0:Validation_Cutoff,:]\n",
    "    TrainTargets = FullTargets[0:Validation_Cutoff,:]\n",
    "\n",
    "    data_dim = TrainInputs.shape[-1]\n",
    "    NumSamples = TrainInputs.shape[0]\n",
    "    Num_Targets = TrainTargets.shape[-1]\n",
    "\n",
    "    index = np.linspace(0,NumSamples,NumSamples,endpoint=False,dtype=int)\n",
    "\n",
    "    reg_coeff = 0.001\n",
    "\n",
    "    def Gen_Model(reg_coeff):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100,activation='relu',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(100,activation='relu',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(100,activation='relu',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "        optim = optimizers.adam(lr=0.0001)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "        return model\n",
    "\n",
    "    D2THard_model = Gen_Model(reg_coeff)\n",
    "    D2TEasy_model = Gen_Model(reg_coeff)\n",
    "    BALDHard_model = Gen_Model(reg_coeff)\n",
    "    BALDEasy_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    Uni_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    D2THard_model.set_weights(Uni_model.get_weights())\n",
    "    D2TEasy_model.set_weights(Uni_model.get_weights())\n",
    "    BALDHard_model.set_weights(Uni_model.get_weights())\n",
    "    BALDEasy_model.set_weights(Uni_model.get_weights())\n",
    "    \n",
    "    def acquisition_function_BALD(model,samples,Num_Targets,temperature=1,Target_Ratio = 5):\n",
    "        nb_MC_samples = 100\n",
    "        MC_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[-1].output])\n",
    "        MC_samples = np.zeros([nb_MC_samples,samples.shape[0],Num_Targets])\n",
    "        learning_phase = True \n",
    "        for i in range(nb_MC_samples):\n",
    "            MC_samples[i,:,:] = np.array([MC_output([samples, learning_phase])[0]])\n",
    "\n",
    "        expected_entropy = - np.mean(np.sum(MC_samples * np.log(MC_samples + 1e-10), axis=-1), axis=0)  # [batch size]\n",
    "        expected_p = np.mean(MC_samples, axis=0)\n",
    "        entropy_expected_p = - np.sum(expected_p * np.log(expected_p + 1e-10), axis=-1)  # [batch size]\n",
    "        BALD_acq = entropy_expected_p - expected_entropy\n",
    "\n",
    "        Exp_BALD = np.exp(BALD_acq/temperature)\n",
    "        Sampling_Prob = Exp_BALD/np.sum(Exp_BALD).astype(float)\n",
    "\n",
    "        Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "        Target_Ratio = Target_Ratio\n",
    "        if Max_Prob_Ratio < Target_Ratio:\n",
    "            while Max_Prob_Ratio <Target_Ratio:\n",
    "                temperature = temperature*0.99\n",
    "                Exp_BALD = np.exp(BALD_acq/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_BALD/np.sum(Exp_BALD).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        else:\n",
    "            while Max_Prob_Ratio > Target_Ratio:\n",
    "                temperature = temperature*1.01\n",
    "                Exp_BALD = np.exp(BALD_acq/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_BALD/np.sum(Exp_BALD).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        return Sampling_Prob\n",
    "\n",
    "    def acquisition_function_dist_to_threshold(model,samples,Num_Targets,temperature=1,Target_Ratio=5):\n",
    "        Output = model.predict(samples)\n",
    "        Output -= 1/float(Num_Targets)\n",
    "        Dist_to_Threshold = np.sum(np.abs(Output),1)\n",
    "        Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "        Exp_Dist_to_Threshold *= 1\n",
    "    #     Exp_Dist_to_Threshold = Dist_to_Threshold\n",
    "        Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "        Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "        Target_Ratio = Target_Ratio\n",
    "        if Max_Prob_Ratio < Target_Ratio:\n",
    "            while Max_Prob_Ratio <Target_Ratio:\n",
    "                temperature = temperature*0.99\n",
    "                Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        else:\n",
    "            while Max_Prob_Ratio > Target_Ratio:\n",
    "                temperature = temperature*1.01\n",
    "                Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        return Sampling_Prob\n",
    "\n",
    "    count = 0 \n",
    "    Batch_Size = 50\n",
    "    Uni_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=Num_Epochs,verbose=1,validation_data=[ValInputs,ValTargets])\n",
    "\n",
    "    Dist_to_Threshold = acquisition_function_BALD(Uni_model,TrainInputs,Num_Targets,1,10)\n",
    "\n",
    "    Easy_Dist_to_Threshold_ArgSort = np.flipud(Dist_to_Threshold.argsort())\n",
    "    Hard_Dist_to_Threshold_ArgSort = Dist_to_Threshold.argsort()\n",
    "\n",
    "    NumTasks = Num_Tasks\n",
    "    TaskSplitPoints = np.int(np.ceil(NumSamples/NumTasks))\n",
    "    print(TaskSplitPoints)\n",
    "    for i in range(NumTasks):\n",
    "        Num_Epochs_Task = Num_Epochs*(1/(i+1))\n",
    "\n",
    "        TaskInd = Hard_Dist_to_Threshold_ArgSort[0:np.min([(i+1)*TaskSplitPoints,NumSamples])]\n",
    "        D2THard_model.fit(TrainInputs[TaskInd,:],TrainTargets[TaskInd],batch_size=Batch_Size,epochs = np.int(Num_Epochs_Task))\n",
    "\n",
    "        TaskInd = Easy_Dist_to_Threshold_ArgSort[0:np.min([(i+1)*TaskSplitPoints,NumSamples])]\n",
    "        D2TEasy_model.fit(TrainInputs[TaskInd,:],TrainTargets[TaskInd],batch_size=Batch_Size,epochs = np.int(Num_Epochs_Task))\n",
    "\n",
    "    TestInputs = scipy.io.loadmat('MNIST_TestInputs.mat')\n",
    "    TestInputs = TestInputs['test_images']\n",
    "\n",
    "    TestTargets = scipy.io.loadmat('MNIST_TestTargets.mat')\n",
    "    TestTargets = TestTargets['test_targets']\n",
    "\n",
    "    TestError = np.zeros([3,2])\n",
    "    TestError[0,:] = D2THard_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[1,:] = D2TEasy_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[2,:] = Uni_model.test_on_batch(TestInputs,TestTargets)\n",
    "    \n",
    "    return TestError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/100\n",
      "34850/45000 [======================>.......] - ETA: 1s - loss: 1.7674 - categorical_accuracy: 0.5365"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-96ef97261219>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mclear_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m     \u001b[0mTestError_Record\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMNIST_Bootstrap_Exp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'MNIST_Bootstrap_Perf_2_BALD'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTestError_Record\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-8a5db958590d>\u001b[0m in \u001b[0;36mMNIST_Bootstrap_Exp\u001b[1;34m(Num_Tasks, Num_Epochs)\u001b[0m\n\u001b[0;32m    178\u001b[0m     \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m     \u001b[0mBatch_Size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m     \u001b[0mUni_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTrainInputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrainTargets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatch_Size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNum_Epochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mValInputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mValTargets\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[0mDist_to_Threshold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0macquisition_function_BALD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mUni_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTrainInputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mNum_Targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ian mcwilliam\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1043\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Users\\ian mcwilliam\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    183\u001b[0m                         \u001b[1;31m# Do not slice the training phase flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[1;32m--> 185\u001b[1;33m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[0;32m    186\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ian mcwilliam\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[1;34m(arrays, start, stop)\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ian mcwilliam\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    505\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    506\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 507\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    508\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    509\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mNone\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "NumTests = 100\n",
    "from IPython.display import clear_output\n",
    "\n",
    "TestError_Record = np.zeros([3,2,NumTests])\n",
    "\n",
    "for i in range(NumTests):\n",
    "    \n",
    "    clear_output()\n",
    "    print('Run:')\n",
    "    print(i)\n",
    "    TestError_Record[:,:,i] = MNIST_Bootstrap_Exp(5,100)\n",
    "    np.save('MNIST_Bootstrap_Perf_2_BALD',TestError_Record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
