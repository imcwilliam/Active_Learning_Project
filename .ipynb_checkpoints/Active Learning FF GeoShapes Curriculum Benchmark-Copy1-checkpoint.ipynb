{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  Run_GeoShapesTests(NumEpochs=300):\n",
    "    %matplotlib notebook\n",
    "    import keras as keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense,BatchNormalization,Dropout,Flatten, Conv1D\n",
    "    from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "    from keras.metrics import categorical_accuracy\n",
    "    from keras import regularizers,optimizers\n",
    "    from keras.regularizers import l2\n",
    "    import numpy as np\n",
    "    import scipy.io\n",
    "    import matplotlib.pyplot as plt\n",
    "    import gzip\n",
    "    from keras.layers.core import Lambda\n",
    "    from scipy.integrate import trapz\n",
    "\n",
    "    from keras.layers.core import Lambda\n",
    "    from keras import backend as K    \n",
    "    \n",
    "    def mini_batches(InputSample,BatchSize):\n",
    "        Index = np.array(range(InputSample.shape[0]),dtype=int)\n",
    "        NumBatches = np.int(InputSample.shape[0]/BatchSize)\n",
    "        Removed = np.array([],dtype=int)\n",
    "\n",
    "        BatchInd =[]\n",
    "        for BatchLoop in range(NumBatches):\n",
    "            RemainIndex = np.delete(Index,Removed)\n",
    "            SampleInd = np.random.choice(RemainIndex,size=BatchSize,replace=False)\n",
    "            Removed = np.append(Removed,SampleInd,axis=0)\n",
    "\n",
    "            BatchInd.append(SampleInd)\n",
    "        RemainIndex = np.delete(Index,Removed)\n",
    "        BatchInd.append(RemainIndex)\n",
    "\n",
    "        return BatchInd,NumBatches\n",
    "\n",
    "    def Get_Feats_and_Targets(filename):\n",
    "        import numpy as np\n",
    "\n",
    "        def line_to_Feats(line):\n",
    "            line = line.split(' ')\n",
    "            Feats = np.asarray(line[0:1024])\n",
    "            Target = np.zeros([3])\n",
    "            Target[int(line[1024])] = 1\n",
    "            return Feats,Target\n",
    "\n",
    "        f = open(filename, 'r')\n",
    "        lines = f.readlines()\n",
    "        Features = []\n",
    "        Targets = []\n",
    "        for i in range(len(lines)-1):\n",
    "            line = lines[i+1]\n",
    "            Feats,Tgts = line_to_Feats(line)\n",
    "            Features.append(Feats)\n",
    "            Targets.append(Tgts)\n",
    "\n",
    "        return np.asarray(Features,dtype = 'float64'), np.asarray(Targets,dtype = 'int')\n",
    "\n",
    "    [TrainInputs_Easy,TrainTargets_Easy] = Get_Feats_and_Targets('shapeset1_1cspo_2_3.10000.train.amat')\n",
    "#     [ValInputs_Easy,ValTargets_Easy] = Get_Feats_and_Targets('shapeset1_1cspo_2_3.5000.valid.amat')\n",
    "\n",
    "    [TrainInputs,TrainTargets] = Get_Feats_and_Targets('shapeset2_1cspo_2_3.10000.train.amat')\n",
    "    [ValInputs,ValTargets] = Get_Feats_and_Targets('shapeset2_1cspo_2_3.5000.valid.amat')\n",
    "\n",
    "    data_dim = TrainInputs.shape[-1]\n",
    "    NumSamples = TrainInputs.shape[0]\n",
    "    Num_Targets = TrainTargets.shape[-1]\n",
    "\n",
    "    index = np.linspace(0,NumSamples,NumSamples,endpoint=False,dtype=int)\n",
    "\n",
    "    reg_coeff = 0.01\n",
    "\n",
    "    def Gen_Model(reg_coeff):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "        optim = optimizers.adagrad(lr=0.005)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "#     ISHard_model = Gen_Model(reg_coeff)\n",
    "\n",
    "#     ISEasy_model = Gen_Model(reg_coeff)\n",
    "    \n",
    "    UniHard_model = Gen_Model(reg_coeff)\n",
    "    \n",
    "    UniEasy_model = Gen_Model(reg_coeff)\n",
    "    \n",
    "#     Easy_model = Gen_Model(reg_coeff)\n",
    "    \n",
    "#     Hard_model =Gen_Model(reg_coeff)\n",
    "\n",
    "    Cur_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    Uni_model = Gen_Model(reg_coeff)\n",
    "    \n",
    "    UniHard_model.set_weights(Uni_model.get_weights())\n",
    "    UniEasy_model.set_weights(Uni_model.get_weights())\n",
    "    Cur_model.set_weights(Uni_model.get_weights())\n",
    "    \n",
    "\n",
    "    def ablate_network(model,ablation_perc):\n",
    "        weights = model.get_weights()\n",
    "        save_weights = weights.copy()\n",
    "        NumLayers = len(weights)\n",
    "        total_numweights  = 0 \n",
    "        for i in range(NumLayers):\n",
    "            layer = weights[i]\n",
    "            total_numweights += len(layer.flatten())\n",
    "        num_ablations = np.int(ablation_perc*total_numweights)\n",
    "        weights = ablate_weights(weights,num_ablations)\n",
    "        model.set_weights(weights)\n",
    "\n",
    "        return(model,save_weights)\n",
    "\n",
    "\n",
    "    def ablation_curve(model,num_tests,max_perc,inputs,targets):\n",
    "        Perc_Space = np.linspace(0,max_perc,num_tests)\n",
    "        Performance = np.zeros(num_tests)\n",
    "        for i in range(num_tests):\n",
    "            model,save_weights = ablate_network(model,Perc_Space[i])\n",
    "            Perf = model.test_on_batch(inputs,targets)\n",
    "            Performance[i] = Perf[1]\n",
    "            model.set_weights(save_weights)\n",
    "\n",
    "            AUC = trapz(Performance,Perc_Space)\n",
    "\n",
    "        return AUC,Performance\n",
    "\n",
    "    \n",
    "    def ablate_weights(weights,num_ablations):\n",
    "        NumLayers = len(weights)\n",
    "        LayerShape = []\n",
    "        LayerShape = []\n",
    "        LayerNodes = [0]\n",
    "        FlattenedNodes = np.empty([0])\n",
    "        for i in range(NumLayers):\n",
    "            LayerShape.append(weights[i].shape)\n",
    "            LayerNodes.append(len(weights[i].flatten()))\n",
    "            FlattenedNodes = np.append(FlattenedNodes,weights[i].flatten())\n",
    "            Index = np.linspace(0,len(FlattenedNodes)-1,num=len(FlattenedNodes),dtype = int)\n",
    "        RandChoice = np.random.choice(Index,size = num_ablations,replace=False)\n",
    "        FlattenedNodes[RandChoice] = 0\n",
    "        FirstInd = 0\n",
    "        for i in range(NumLayers):\n",
    "            FirstInd += LayerNodes[i]\n",
    "            SecondInd = FirstInd + LayerNodes[i+1]\n",
    "            Sample = FlattenedNodes[FirstInd:SecondInd]\n",
    "            Sample = Sample.reshape(LayerShape[i])\n",
    "            weights[i] = Sample\n",
    "\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def acquisition_function_ablation(model,Inputs,Targets,Cluster_Size):\n",
    "        BatchInd,NumBatches = mini_batches(Inputs,Cluster_Size)\n",
    "        AUC_Record = np.zeros([Inputs.shape[0]])\n",
    "        count= 0\n",
    "        for Batch in BatchInd:\n",
    "            if Batch.shape[0] != 0:\n",
    "                BatchInputs = Inputs[Batch,:]\n",
    "                BatchTargets = Targets[Batch,:]\n",
    "                SaveWeights = model.get_weights()\n",
    "                model.fit(BatchInputs,BatchTargets,batch_size=64,verbose=0)\n",
    "                AUC,_ = ablation_curve(model,30,1,Inputs,Targets)\n",
    "                AUC_Record[Batch] = AUC.copy()\n",
    "                model.set_weights(SaveWeights)\n",
    "                count +=1\n",
    "\n",
    "        Sampling_Prob = AUC_Record/AUC_Record.sum()\n",
    "        return Sampling_Prob\n",
    "\n",
    "    def acquisition_function_dist_to_threshold(model,samples,Num_Targets,Smoothing_Const=0,Rescale=0):\n",
    "        Output = model.predict(samples)\n",
    "        Output -= 1/float(Num_Targets)\n",
    "        Dist_to_Threshold = np.sum(np.abs(Output),1)\n",
    "        Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold) + Smoothing_Const\n",
    "        Exp_Dist_to_Threshold *= 1\n",
    "        Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "        if Rescale == 1:\n",
    "            Median_Prob = np.median(Sampling_Prob)\n",
    "            Min_Prob = np.min(Sampling_Prob)\n",
    "            Sampling_Prob[Sampling_Prob > Median_Prob] += Min_Prob/2\n",
    "            Sampling_Prob[Sampling_Prob < Median_Prob] -= Min_Prob/2\n",
    "        return Sampling_Prob\n",
    "    \n",
    "    def Exp_ModelChange(Model,Inputs,Num_Targets):\n",
    "        ExpChange = np.zeros(Inputs.shape[0])\n",
    "        for TargetLoop in range(Num_Targets):\n",
    "            thisTarget = np.zeros([Inputs.shape[0],Num_Targets])\n",
    "            thisTarget[:,TargetLoop] = 1\n",
    "            Model_Output = Model.predict(Inputs)\n",
    "            ClassProb = Model_Output[:,TargetLoop]\n",
    "            Loss = np.mean(Model_Output - Model_Output*thisTarget + np.log(1+np.exp(-Model_Output)),1)\n",
    "            ExpChange += Loss*ClassProb\n",
    "        return ExpChange\n",
    "\n",
    "    def acquisition_function_exp_model_change(model,samples,Num_Targets,Smoothing_Const=0,Rescale=0):\n",
    "        ExpChange = Exp_ModelChange(model,samples,Num_Targets) + Smoothing_Const\n",
    "        Sampling_Prob = ExpChange/np.sum(ExpChange)\n",
    "        if Rescale == 1:\n",
    "            Median_Prob = np.median(Sampling_Prob)\n",
    "            Min_Prob = np.min(Sampling_Prob)\n",
    "            Sampling_Prob[Sampling_Prob > Median_Prob] += Min_Prob/2\n",
    "            Sampling_Prob[Sampling_Prob < Median_Prob] -= Min_Prob/2\n",
    "        return Sampling_Prob\n",
    "\n",
    "    count = 0 \n",
    "    Num_Epochs = NumEpochs\n",
    "    Num_BurnIn = 5\n",
    "    Switch_Epoch = 128\n",
    "    Batch_Size = 32\n",
    "\n",
    "    Smoothing_Constant = 0\n",
    "\n",
    "    Val_Error = np.zeros([Num_Epochs,4])\n",
    "    Val_Acc = np.zeros([Num_Epochs,4])\n",
    "\n",
    "    def Biased_Batch(Inputs,Targets,Batch_Size,Sampling_Probability,Weighting_Flag = 1):\n",
    "        while True: \n",
    "            index = np.linspace(0,Inputs.shape[0],Inputs.shape[0],endpoint=False,dtype=int)\n",
    "            Batch = np.random.choice(index,size=Batch_Size,replace=False,p=Sampling_Probability)\n",
    "            Batch_Inputs = Inputs[Batch,:]\n",
    "            Batch_Targets = Targets[Batch,:]\n",
    "            if Weighting_Flag == 1:\n",
    "                FullWeights = Sampling_Probability**-1\n",
    "                FullWeights = FullWeights/np.mean(FullWeights)\n",
    "            elif Weighting_Flag == -1:\n",
    "                FullWeights = Sampling_Probability/np.mean(Sampling_Probability)\n",
    "            else:\n",
    "                FullWeights = np.ones(Inputs.shape[0])\n",
    "\n",
    "            Weights = FullWeights[Batch]\n",
    "            yield (Batch_Inputs,Batch_Targets,Weights)\n",
    "\n",
    "\n",
    "    for EpochLoop in  range(Num_Epochs):\n",
    "        if count < Num_BurnIn:\n",
    "#             ISHard_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "#             Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,0] = Error[0]\n",
    "#             Val_Acc[count,0] = Error[1]\n",
    "\n",
    "#             ISEasy_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "#             Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,1] = Error[0]\n",
    "#             Val_Acc[count,1] = Error[1]\n",
    "            \n",
    "#             Hard_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "#             Error = Hard_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,2] = Error[0]\n",
    "#             Val_Acc[count,2] = Error[1]\n",
    "\n",
    "#             Easy_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "#             Error = Easy_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,3] = Error[0]\n",
    "#             Val_Acc[count,3] = Error[1]\n",
    "            \n",
    "            UniHard_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "            Error = UniHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,0] = Error[0]\n",
    "            Val_Acc[count,0] = Error[1]\n",
    "\n",
    "            UniEasy_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "            Error = UniEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,1] = Error[0]\n",
    "            Val_Acc[count,1] = Error[1]\n",
    "\n",
    "            Uni_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "            Error = Uni_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,2] = Error[0]\n",
    "            Val_Acc[count,2] = Error[1]\n",
    "\n",
    "            if count < Switch_Epoch:\n",
    "                Cur_model.fit(TrainInputs_Easy,TrainTargets_Easy,batch_size = 32,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "                Error = Cur_model.test_on_batch(ValInputs,ValTargets)\n",
    "                Val_Error[count,3] = Error[0]\n",
    "                Val_Acc[count,3] = Error[1]\n",
    "            else:\n",
    "                Cur_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "                Error = Cur_model.test_on_batch(ValInputs,ValTargets)\n",
    "                Val_Error[count,3] = Error[0]\n",
    "                Val_Acc[count,3] = Error[1]\n",
    "\n",
    "\n",
    "        else:\n",
    "#             ISHard_Sampling_Probability = acquisition_function_dist_to_threshold(ISHard_model,TrainInputs,Num_Targets,0,0)\n",
    "#             ISEasy_SamplingProbability = 1/acquisition_function_dist_to_threshold(ISEasy_model,TrainInputs,Num_Targets,0,0)\n",
    "#             ISEasy_SamplingProbability/= ISEasy_SamplingProbability.sum().astype(float)\n",
    "            \n",
    "#             Hard_Sampling_Probability = acquisition_function_dist_to_threshold(Hard_model,TrainInputs,Num_Targets,0,0)\n",
    "#             Easy_SamplingProbability = 1/acquisition_function_dist_to_threshold(Easy_model,TrainInputs,Num_Targets,0,0)\n",
    "#             Easy_SamplingProbability/= Easy_SamplingProbability.sum().astype(float)\n",
    "            \n",
    "            UniHard_Sampling_Probability = acquisition_function_dist_to_threshold(UniHard_model,TrainInputs,Num_Targets,0,1)\n",
    "            UniEasy_Sampling_Probability = 1/acquisition_function_dist_to_threshold(UniEasy_model,TrainInputs,Num_Targets,0,1)\n",
    "            UniEasy_Sampling_Probability /= UniEasy_Sampling_Probability.sum().astype(float)\n",
    "\n",
    "#             ISHard_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,ISHard_Sampling_Probability,1),\n",
    "#                                      steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "#             ISEasy_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,ISEasy_SamplingProbability,1),\n",
    "#                                      steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "            \n",
    "#             Hard_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,Hard_Sampling_Probability,-1),\n",
    "#                                      steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "#             Easy_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,Easy_SamplingProbability,-1),\n",
    "#                                      steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "            \n",
    "            UniHard_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,UniHard_Sampling_Probability,-1),\n",
    "                                     steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "            UniEasy_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,UniEasy_Sampling_Probability,-1),\n",
    "                                     steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "            Uni_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,1/(NumSamples*np.ones(NumSamples))),\n",
    "                                    steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "            if count < Switch_Epoch:\n",
    "                Cur_model.fit(TrainInputs_Easy,TrainTargets_Easy,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "            else:\n",
    "                Cur_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "\n",
    "\n",
    "#             Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,0] = Error[0]\n",
    "#             Val_Acc[count,0] = Error[1]\n",
    "\n",
    "#             Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,1] = Error[0]\n",
    "#             Val_Acc[count,1] = Error[1]\n",
    "            \n",
    "#             Error = Hard_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,2] = Error[0]\n",
    "#             Val_Acc[count,2] = Error[1]\n",
    "\n",
    "#             Error = Easy_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,3] = Error[0]\n",
    "#             Val_Acc[count,3] = Error[1]\n",
    "            \n",
    "            Error = UniHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,0] = Error[0]\n",
    "            Val_Acc[count,0] = Error[1]\n",
    "\n",
    "            Error = UniEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,1] = Error[0]\n",
    "            Val_Acc[count,1] = Error[1]\n",
    "\n",
    "            Error = Uni_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,2] = Error[0]\n",
    "            Val_Acc[count,2] = Error[1]\n",
    "\n",
    "            Error = Cur_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,3] = Error[0]\n",
    "            Val_Acc[count,3] = Error[1]\n",
    "\n",
    "        count += 1\n",
    "\n",
    "\n",
    "    [TestInputs,TestTargets] = Get_Feats_and_Targets('shapeset2_1cspo_2_3.5000.test.amat')\n",
    "\n",
    "    TestError = np.zeros([4,2])\n",
    "#     TestError[0,:] = ISHard_model.test_on_batch(TestInputs,TestTargets)\n",
    "#     TestError[1,:] = ISEasy_model.test_on_batch(TestInputs,TestTargets)\n",
    "#     TestError[2,:] = Hard_model.test_on_batch(TestInputs,TestTargets)\n",
    "#     TestError[3,:] = Easy_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[0,:] = UniHard_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[1,:] = UniEasy_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[2,:] = Uni_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[3,:] = Cur_model.test_on_batch(TestInputs,TestTargets)\n",
    "    \n",
    "    return Val_Error,Val_Acc,TestError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "NumTests = 1000\n",
    "NumEpochs = 256\n",
    "Val_Error_Record = np.zeros([NumEpochs,4,NumTests])\n",
    "Val_Acc_Record = np.zeros([NumEpochs,4,NumTests])\n",
    "Test_Perf_Record = np.zeros([4,2,NumTests])\n",
    "\n",
    "for TestLoop in range(NumTests):\n",
    "    print('Test Number:')\n",
    "    print(TestLoop)\n",
    "    Val_Error_Record[:,:,TestLoop],Val_Acc_Record[:,:,TestLoop],Test_Perf_Record[:,:,TestLoop] = Run_GeoShapesTests(NumEpochs)\n",
    "    \n",
    "    DateString = str(datetime.datetime.date(datetime.datetime.now()))\n",
    "    \n",
    "    np.save(\"Test_Perf_Record_CurriculumBaseline_256Epoch_Tanh_0.001l2_0.001LR_adagrad_justUni_ablate\",Test_Perf_Record)\n",
    "    np.save(\"Val_Error_Record_CurriculumBaseline_256Epoch_Tanh_0.001l2_0.001LR_adagrad_justUni_ablate\",Val_Error_Record)\n",
    "    np.save(\"Val_Acc_Record_CurriculumBaseline_256Epoch_Tanh_0.001l2_0.001LR_adagrad_justUni_ablate\",Val_Acc_Record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b,c = Run_GeoShapesTests(NumEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "thisdate =  str(datetime.datetime.date(datetime.datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-06-11'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thisdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import relu\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.regularizers import l2\n",
    "data_dim = 10\n",
    "reg_coeff = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(100,input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "model.add(Activation(relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense,BatchNormalization,Dropout,Flatten, Conv1D\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras import regularizers,optimizers\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "from keras.layers.core import Lambda\n",
    "from scipy.integrate import trapz\n",
    "\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (<ipython-input-4-1a63bc91cc64>, line 86)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-1a63bc91cc64>\"\u001b[1;36m, line \u001b[1;32m86\u001b[0m\n\u001b[1;33m    weights = model.get_weights()\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "NumEpochs=300\n",
    "%matplotlib notebook\n",
    "import keras as keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense,BatchNormalization,Dropout,Flatten, Conv1D\n",
    "from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras import regularizers,optimizers\n",
    "from keras.regularizers import l2\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "from keras.layers.core import Lambda\n",
    "from scipy.integrate import trapz\n",
    "\n",
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K    \n",
    "\n",
    "def Get_Feats_and_Targets(filename):\n",
    "    import numpy as np\n",
    "\n",
    "    def line_to_Feats(line):\n",
    "        line = line.split(' ')\n",
    "        Feats = np.asarray(line[0:1024])\n",
    "        Target = np.zeros([3])\n",
    "        Target[int(line[1024])] = 1\n",
    "        return Feats,Target\n",
    "\n",
    "    f = open(filename, 'r')\n",
    "    lines = f.readlines()\n",
    "    Features = []\n",
    "    Targets = []\n",
    "    for i in range(len(lines)-1):\n",
    "        line = lines[i+1]\n",
    "        Feats,Tgts = line_to_Feats(line)\n",
    "        Features.append(Feats)\n",
    "        Targets.append(Tgts)\n",
    "\n",
    "    return np.asarray(Features,dtype = 'float64'), np.asarray(Targets,dtype = 'int')\n",
    "\n",
    "[TrainInputs_Easy,TrainTargets_Easy] = Get_Feats_and_Targets('shapeset1_1cspo_2_3.10000.train.amat')\n",
    "#     [ValInputs_Easy,ValTargets_Easy] = Get_Feats_and_Targets('shapeset1_1cspo_2_3.5000.valid.amat')\n",
    "\n",
    "[TrainInputs,TrainTargets] = Get_Feats_and_Targets('shapeset2_1cspo_2_3.10000.train.amat')\n",
    "[ValInputs,ValTargets] = Get_Feats_and_Targets('shapeset2_1cspo_2_3.5000.valid.amat')\n",
    "\n",
    "data_dim = TrainInputs.shape[-1]\n",
    "NumSamples = TrainInputs.shape[0]\n",
    "Num_Targets = TrainTargets.shape[-1]\n",
    "\n",
    "index = np.linspace(0,NumSamples,NumSamples,endpoint=False,dtype=int)\n",
    "\n",
    "reg_coeff = 0.001\n",
    "\n",
    "def Gen_Model(reg_coeff):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "    model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "    model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "    model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "    optim = optimizers.adagrad(lr=0.001)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "#     ISHard_model = Gen_Model(reg_coeff)\n",
    "\n",
    "#     ISEasy_model = Gen_Model(reg_coeff)\n",
    "\n",
    "UniHard_model = Gen_Model(reg_coeff)\n",
    "\n",
    "UniEasy_model = Gen_Model(reg_coeff)\n",
    "\n",
    "#     Easy_model = Gen_Model(reg_coeff)\n",
    "\n",
    "#     Hard_model =Gen_Model(reg_coeff)\n",
    "\n",
    "Cur_model = Gen_Model(reg_coeff)\n",
    "\n",
    "Uni_model = Gen_Model(reg_coeff)\n",
    "\n",
    "\n",
    "\n",
    "def ablate_network(model,ablation_perc):\n",
    "weights = model.get_weights()\n",
    "save_weights = weights.copy()\n",
    "NumLayers = len(weights)\n",
    "total_numweights  = 0 \n",
    "for i in range(NumLayers):\n",
    "    layer = weights[i]\n",
    "    total_numweights += len(layer.flatten())\n",
    "num_ablations = np.int(ablation_perc*total_numweights)\n",
    "weights = ablate_weights(weights,num_ablations)\n",
    "model.set_weights(weights)\n",
    "\n",
    "return(model,save_weights)\n",
    "\n",
    "\n",
    "def ablation_curve(model,num_tests,max_perc,inputs,targets):\n",
    "Perc_Space = np.linspace(0,max_perc,num_tests)\n",
    "Performance = np.zeros(num_tests)\n",
    "for i in range(num_tests):\n",
    "    model,save_weights = ablate_network(model,Perc_Space[i])\n",
    "    Perf = model.test_on_batch(inputs,targets)\n",
    "    Performance[i] = Perf[1]\n",
    "    model.set_weights(save_weights)\n",
    "\n",
    "    AUC = trapz(Performance,Perc_Space)\n",
    "\n",
    "return AUC,Performance\n",
    "\n",
    "\n",
    "def ablate_weights(weights,num_ablations):\n",
    "NumLayers = len(weights)\n",
    "LayerShape = []\n",
    "LayerShape = []\n",
    "LayerNodes = [0]\n",
    "FlattenedNodes = np.empty([0])\n",
    "for i in range(NumLayers):\n",
    "    LayerShape.append(weights[i].shape)\n",
    "    LayerNodes.append(len(weights[i].flatten()))\n",
    "    FlattenedNodes = np.append(FlattenedNodes,weights[i].flatten())\n",
    "    Index = np.linspace(0,len(FlattenedNodes)-1,num=len(FlattenedNodes),dtype = int)\n",
    "RandChoice = np.random.choice(Index,size = num_ablations,replace=False)\n",
    "FlattenedNodes[RandChoice] = 0\n",
    "FirstInd = 0\n",
    "for i in range(NumLayers):\n",
    "    FirstInd += LayerNodes[i]\n",
    "    SecondInd = FirstInd + LayerNodes[i+1]\n",
    "    Sample = FlattenedNodes[FirstInd:SecondInd]\n",
    "    Sample = Sample.reshape(LayerShape[i])\n",
    "    weights[i] = Sample\n",
    "\n",
    "return weights\n",
    "\n",
    "\n",
    "def acquisition_function_ablation(model,Inputs,Targets,Cluster_Size):\n",
    "BatchInd,NumBatches = mini_batches(Inputs,Cluster_Size)\n",
    "AUC_Record = np.zeros([Inputs.shape[0]])\n",
    "count= 0\n",
    "for Batch in BatchInd:\n",
    "    if Batch.shape[0] != 0:\n",
    "        BatchInputs = Inputs[Batch,:]\n",
    "        BatchTargets = Targets[Batch,:]\n",
    "        SaveWeights = model.get_weights()\n",
    "        Hard_model.fit(BatchInputs,BatchTargets,batch_size=64,verbose=0)\n",
    "        AUC,_ = ablation_curve(model,30,1,Inputs,Targets)\n",
    "        AUC_Record[Batch] = AUC.copy()\n",
    "        Hard_model.set_weights(SaveWeights)\n",
    "        count +=1\n",
    "\n",
    "Sampling_Prob = AUC_Record/AUC_Record.sum()\n",
    "return Sampling_Prob\n",
    "\n",
    "def acquisition_function_dist_to_threshold(model,samples,Num_Targets,Smoothing_Const=0,Rescale=0):\n",
    "    Output = model.predict(samples)\n",
    "    Output -= 1/Num_Targets\n",
    "    Dist_to_Threshold = np.sum(np.abs(Output),1)\n",
    "    Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold) + Smoothing_Const\n",
    "    Exp_Dist_to_Threshold *= 1\n",
    "    Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "    if Rescale == 1:\n",
    "        Median_Prob = np.median(Sampling_Prob)\n",
    "        Min_Prob = np.min(Sampling_Prob)\n",
    "        Sampling_Prob[Sampling_Prob > Median_Prob] += Min_Prob/2\n",
    "        Sampling_Prob[Sampling_Prob < Median_Prob] -= Min_Prob/2\n",
    "    return Sampling_Prob\n",
    "\n",
    "def Exp_ModelChange(Model,Inputs,Num_Targets):\n",
    "    ExpChange = np.zeros(Inputs.shape[0])\n",
    "    for TargetLoop in range(Num_Targets):\n",
    "        thisTarget = np.zeros([Inputs.shape[0],Num_Targets])\n",
    "        thisTarget[:,TargetLoop] = 1\n",
    "        Model_Output = Model.predict(Inputs)\n",
    "        ClassProb = Model_Output[:,TargetLoop]\n",
    "        Loss = np.mean(Model_Output - Model_Output*thisTarget + np.log(1+np.exp(-Model_Output)),1)\n",
    "        ExpChange += Loss*ClassProb\n",
    "    return ExpChange\n",
    "\n",
    "def acquisition_function_exp_model_change(model,samples,Num_Targets,Smoothing_Const=0,Rescale=0):\n",
    "    ExpChange = Exp_ModelChange(model,samples,Num_Targets) + Smoothing_Const\n",
    "    Sampling_Prob = ExpChange/np.sum(ExpChange)\n",
    "    if Rescale == 1:\n",
    "        Median_Prob = np.median(Sampling_Prob)\n",
    "        Min_Prob = np.min(Sampling_Prob)\n",
    "        Sampling_Prob[Sampling_Prob > Median_Prob] += Min_Prob/2\n",
    "        Sampling_Prob[Sampling_Prob < Median_Prob] -= Min_Prob/2\n",
    "    return Sampling_Prob\n",
    "\n",
    "count = 0 \n",
    "Num_Epochs = NumEpochs\n",
    "Num_BurnIn = 5\n",
    "Switch_Epoch = 128\n",
    "Batch_Size = 32\n",
    "\n",
    "Smoothing_Constant = 0\n",
    "\n",
    "Val_Error = np.zeros([Num_Epochs,4])\n",
    "Val_Acc = np.zeros([Num_Epochs,4])\n",
    "\n",
    "def Biased_Batch(Inputs,Targets,Batch_Size,Sampling_Probability,Weighting_Flag = 1):\n",
    "    while True: \n",
    "        index = np.linspace(0,Inputs.shape[0],Inputs.shape[0],endpoint=False,dtype=int)\n",
    "        Batch = np.random.choice(index,size=Batch_Size,replace=False,p=Sampling_Probability)\n",
    "        Batch_Inputs = Inputs[Batch,:]\n",
    "        Batch_Targets = Targets[Batch,:]\n",
    "        if Weighting_Flag == 1:\n",
    "            FullWeights = Sampling_Probability**-1\n",
    "            FullWeights = FullWeights/np.mean(FullWeights)\n",
    "        elif Weighting_Flag == -1:\n",
    "            FullWeights = Sampling_Probability/np.mean(Sampling_Probability)\n",
    "        else:\n",
    "            FullWeights = np.ones(Inputs.shape[0])\n",
    "\n",
    "        Weights = FullWeights[Batch]\n",
    "        yield (Batch_Inputs,Batch_Targets,Weights)\n",
    "\n",
    "\n",
    "for EpochLoop in  range(Num_Epochs):\n",
    "    if count < Num_BurnIn:\n",
    "#             ISHard_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "#             Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,0] = Error[0]\n",
    "#             Val_Acc[count,0] = Error[1]\n",
    "\n",
    "#             ISEasy_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "#             Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,1] = Error[0]\n",
    "#             Val_Acc[count,1] = Error[1]\n",
    "\n",
    "#             Hard_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "#             Error = Hard_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,2] = Error[0]\n",
    "#             Val_Acc[count,2] = Error[1]\n",
    "\n",
    "#             Easy_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "#             Error = Easy_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,3] = Error[0]\n",
    "#             Val_Acc[count,3] = Error[1]\n",
    "\n",
    "        UniHard_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "        Error = UniHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "        Val_Error[count,0] = Error[0]\n",
    "        Val_Acc[count,0] = Error[1]\n",
    "\n",
    "        UniEasy_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "        Error = UniEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "        Val_Error[count,1] = Error[0]\n",
    "        Val_Acc[count,1] = Error[1]\n",
    "\n",
    "        Uni_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "        Error = Uni_model.test_on_batch(ValInputs,ValTargets)\n",
    "        Val_Error[count,2] = Error[0]\n",
    "        Val_Acc[count,2] = Error[1]\n",
    "\n",
    "        if count < Switch_Epoch:\n",
    "            Cur_model.fit(TrainInputs_Easy,TrainTargets_Easy,batch_size = 32,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "            Error = Cur_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,3] = Error[0]\n",
    "            Val_Acc[count,3] = Error[1]\n",
    "        else:\n",
    "            Cur_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "            Error = Cur_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,3] = Error[0]\n",
    "            Val_Acc[count,3] = Error[1]\n",
    "\n",
    "\n",
    "    else:\n",
    "#             ISHard_Sampling_Probability = acquisition_function_dist_to_threshold(ISHard_model,TrainInputs,Num_Targets,0,0)\n",
    "#             ISEasy_SamplingProbability = 1/acquisition_function_dist_to_threshold(ISEasy_model,TrainInputs,Num_Targets,0,0)\n",
    "#             ISEasy_SamplingProbability/= ISEasy_SamplingProbability.sum().astype(float)\n",
    "\n",
    "#             Hard_Sampling_Probability = acquisition_function_dist_to_threshold(Hard_model,TrainInputs,Num_Targets,0,0)\n",
    "#             Easy_SamplingProbability = 1/acquisition_function_dist_to_threshold(Easy_model,TrainInputs,Num_Targets,0,0)\n",
    "#             Easy_SamplingProbability/= Easy_SamplingProbability.sum().astype(float)\n",
    "\n",
    "        UniHard_Sampling_Probability = acquisition_function_ablation(UniHard_model,TrainInputs,Num_Targets,32)\n",
    "        UniEasy_Sampling_Probability = 1/acquisition_function_ablation(UniEasy_model,TrainInputs,Num_Targets,32)\n",
    "        UniEasy_Sampling_Probability /= UniEasy_Sampling_Probability.sum().astype(float)\n",
    "\n",
    "#             ISHard_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,ISHard_Sampling_Probability,1),\n",
    "#                                      steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "#             ISEasy_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,ISEasy_SamplingProbability,1),\n",
    "#                                      steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "#             Hard_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,Hard_Sampling_Probability,-1),\n",
    "#                                      steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "#             Easy_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,Easy_SamplingProbability,-1),\n",
    "#                                      steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "        UniHard_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,UniHard_Sampling_Probability,0),\n",
    "                                 steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "        UniEasy_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,UniEasy_Sampling_Probability,0),\n",
    "                                 steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "        Uni_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,1/(NumSamples*np.ones(NumSamples))),\n",
    "                                steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "        if count < Switch_Epoch:\n",
    "            Cur_model.fit(TrainInputs_Easy,TrainTargets_Easy,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "        else:\n",
    "            Cur_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "\n",
    "\n",
    "#             Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,0] = Error[0]\n",
    "#             Val_Acc[count,0] = Error[1]\n",
    "\n",
    "#             Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,1] = Error[0]\n",
    "#             Val_Acc[count,1] = Error[1]\n",
    "\n",
    "#             Error = Hard_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,2] = Error[0]\n",
    "#             Val_Acc[count,2] = Error[1]\n",
    "\n",
    "#             Error = Easy_model.test_on_batch(ValInputs,ValTargets)\n",
    "#             Val_Error[count,3] = Error[0]\n",
    "#             Val_Acc[count,3] = Error[1]\n",
    "\n",
    "        Error = UniHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "        Val_Error[count,0] = Error[0]\n",
    "        Val_Acc[count,0] = Error[1]\n",
    "\n",
    "        Error = UniEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "        Val_Error[count,1] = Error[0]\n",
    "        Val_Acc[count,1] = Error[1]\n",
    "\n",
    "        Error = Uni_model.test_on_batch(ValInputs,ValTargets)\n",
    "        Val_Error[count,2] = Error[0]\n",
    "        Val_Acc[count,2] = Error[1]\n",
    "\n",
    "        Error = Cur_model.test_on_batch(ValInputs,ValTargets)\n",
    "        Val_Error[count,3] = Error[0]\n",
    "        Val_Acc[count,3] = Error[1]\n",
    "\n",
    "    count += 1\n",
    "\n",
    "\n",
    "[TestInputs,TestTargets] = Get_Feats_and_Targets('shapeset2_1cspo_2_3.5000.test.amat')\n",
    "\n",
    "TestError = np.zeros([4,2])\n",
    "#     TestError[0,:] = ISHard_model.test_on_batch(TestInputs,TestTargets)\n",
    "#     TestError[1,:] = ISEasy_model.test_on_batch(TestInputs,TestTargets)\n",
    "#     TestError[2,:] = Hard_model.test_on_batch(TestInputs,TestTargets)\n",
    "#     TestError[3,:] = Easy_model.test_on_batch(TestInputs,TestTargets)\n",
    "TestError[0,:] = UniHard_model.test_on_batch(TestInputs,TestTargets)\n",
    "TestError[1,:] = UniEasy_model.test_on_batch(TestInputs,TestTargets)\n",
    "TestError[2,:] = Uni_model.test_on_batch(TestInputs,TestTargets)\n",
    "TestError[3,:] = Cur_model.test_on_batch(TestInputs,TestTargets)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
