{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MNIST_Bootstrap_Exp(Num_Tasks,Num_Epochs):\n",
    "    %matplotlib notebook\n",
    "    import keras as keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense,BatchNormalization,Dropout,Flatten, Conv1D\n",
    "    from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "    from keras.metrics import categorical_accuracy\n",
    "    from keras import regularizers,optimizers\n",
    "    from keras.regularizers import l2\n",
    "    import numpy as np\n",
    "    import scipy.io\n",
    "    import matplotlib.pyplot as plt\n",
    "    import gzip\n",
    "    from keras.layers.core import Lambda\n",
    "    from scipy.integrate import trapz\n",
    "    import seaborn as sns\n",
    "\n",
    "    from keras.layers.core import Lambda\n",
    "    from keras import backend as K    \n",
    "\n",
    "\n",
    "    def mini_batches(InputSample,BatchSize):\n",
    "        Index = np.array(range(InputSample.shape[0]),dtype=int)\n",
    "        NumBatches = np.int(InputSample.shape[0]/BatchSize)\n",
    "        Removed = np.array([],dtype=int)\n",
    "\n",
    "        BatchInd =[]\n",
    "        for BatchLoop in range(NumBatches):\n",
    "            RemainIndex = np.delete(Index,Removed)\n",
    "            SampleInd = np.random.choice(RemainIndex,size=BatchSize,replace=False)\n",
    "            Removed = np.append(Removed,SampleInd,axis=0)\n",
    "\n",
    "            BatchInd.append(SampleInd)\n",
    "        RemainIndex = np.delete(Index,Removed)\n",
    "        BatchInd.append(RemainIndex)\n",
    "\n",
    "        return BatchInd,NumBatches\n",
    "\n",
    "    def Get_Feats_and_Targets(filename):\n",
    "        import numpy as np\n",
    "\n",
    "        def line_to_Feats(line):\n",
    "            line = line.split(' ')\n",
    "            Feats = np.asarray(line[0:1024])\n",
    "            Target = np.zeros([3])\n",
    "            Target[int(line[1024])] = 1\n",
    "            return Feats,Target\n",
    "\n",
    "        f = open(filename, 'r')\n",
    "        lines = f.readlines()\n",
    "        Features = []\n",
    "        Targets = []\n",
    "        for i in range(len(lines)-1):\n",
    "            line = lines[i+1]\n",
    "            Feats,Tgts = line_to_Feats(line)\n",
    "            Features.append(Feats)\n",
    "            Targets.append(Tgts)\n",
    "\n",
    "        return np.asarray(Features,dtype = 'float64'), np.asarray(Targets,dtype = 'int')\n",
    "\n",
    "    FullInputs = scipy.io.loadmat('MNIST_TrainInputs.mat')\n",
    "    FullInputs = FullInputs['images']\n",
    "\n",
    "    FullTargets = scipy.io.loadmat('MNIST_TrainTargets.mat')\n",
    "    FullTargets = FullTargets['targets']\n",
    "\n",
    "    Validation_Cutoff = 0.75\n",
    "\n",
    "    Validation_Cutoff = np.int(Validation_Cutoff*FullInputs.shape[0])\n",
    "\n",
    "    ValInputs = FullInputs[Validation_Cutoff:,:]\n",
    "    ValTargets = FullTargets[Validation_Cutoff:,:]\n",
    "\n",
    "    TrainInputs = FullInputs[0:Validation_Cutoff,:]\n",
    "    TrainTargets = FullTargets[0:Validation_Cutoff,:]\n",
    "\n",
    "    data_dim = TrainInputs.shape[-1]\n",
    "    NumSamples = TrainInputs.shape[0]\n",
    "    Num_Targets = TrainTargets.shape[-1]\n",
    "\n",
    "    index = np.linspace(0,NumSamples,NumSamples,endpoint=False,dtype=int)\n",
    "\n",
    "    reg_coeff = 0.001\n",
    "\n",
    "    def Gen_Model(reg_coeff):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(100,activation='relu',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(100,activation='relu',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(100,activation='relu',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "        optim = optimizers.adam(lr=0.0001)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "        return model\n",
    "\n",
    "    D2THard_model = Gen_Model(reg_coeff)\n",
    "    D2TEasy_model = Gen_Model(reg_coeff)\n",
    "    BALDHard_model = Gen_Model(reg_coeff)\n",
    "    BALDEasy_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    Uni_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    D2THard_model.set_weights(Uni_model.get_weights())\n",
    "    D2TEasy_model.set_weights(Uni_model.get_weights())\n",
    "    BALDHard_model.set_weights(Uni_model.get_weights())\n",
    "    BALDEasy_model.set_weights(Uni_model.get_weights())\n",
    "    \n",
    "    def acquisition_function_BALD(model,samples,Num_Targets,temperature=1,Target_Ratio = 5):\n",
    "        nb_MC_samples = 100\n",
    "        MC_output = K.function([model.layers[0].input, K.learning_phase()], [model.layers[-1].output])\n",
    "        MC_samples = np.zeros([nb_MC_samples,samples.shape[0],Num_Targets])\n",
    "        learning_phase = True \n",
    "        for i in range(nb_MC_samples):\n",
    "            MC_samples[i,:,:] = np.array([MC_output([samples, learning_phase])[0]])\n",
    "\n",
    "        expected_entropy = - np.mean(np.sum(MC_samples * np.log(MC_samples + 1e-10), axis=-1), axis=0)  # [batch size]\n",
    "        expected_p = np.mean(MC_samples, axis=0)\n",
    "        entropy_expected_p = - np.sum(expected_p * np.log(expected_p + 1e-10), axis=-1)  # [batch size]\n",
    "        BALD_acq = entropy_expected_p - expected_entropy\n",
    "\n",
    "        Exp_BALD = np.exp(BALD_acq/temperature)\n",
    "        Sampling_Prob = Exp_BALD/np.sum(Exp_BALD).astype(float)\n",
    "\n",
    "        Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "        Target_Ratio = Target_Ratio\n",
    "        if Max_Prob_Ratio < Target_Ratio:\n",
    "            while Max_Prob_Ratio <Target_Ratio:\n",
    "                temperature = temperature*0.99\n",
    "                Exp_BALD = np.exp(BALD_acq/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_BALD/np.sum(Exp_BALD).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        else:\n",
    "            while Max_Prob_Ratio > Target_Ratio:\n",
    "                temperature = temperature*1.01\n",
    "                Exp_BALD = np.exp(BALD_acq/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_BALD/np.sum(Exp_BALD).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        return Sampling_Prob\n",
    "\n",
    "    def acquisition_function_dist_to_threshold(model,samples,Num_Targets,temperature=1,Target_Ratio=5):\n",
    "        Output = model.predict(samples)\n",
    "        Output -= 1/float(Num_Targets)\n",
    "        Dist_to_Threshold = np.sum(np.abs(Output),1)\n",
    "        Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "        Exp_Dist_to_Threshold *= 1\n",
    "    #     Exp_Dist_to_Threshold = Dist_to_Threshold\n",
    "        Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "        Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "        Target_Ratio = Target_Ratio\n",
    "        if Max_Prob_Ratio < Target_Ratio:\n",
    "            while Max_Prob_Ratio <Target_Ratio:\n",
    "                temperature = temperature*0.99\n",
    "                Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        else:\n",
    "            while Max_Prob_Ratio > Target_Ratio:\n",
    "                temperature = temperature*1.01\n",
    "                Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "                StoreSampling_Prob = Sampling_Prob.copy()\n",
    "                Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "                Max_Prob_Ratio = Sampling_Prob.max()/Sampling_Prob.min()\n",
    "                if np.isnan(Max_Prob_Ratio):\n",
    "                    Sampling_Prob = StoreSampling_Prob.copy()\n",
    "        return Sampling_Prob\n",
    "\n",
    "    count = 0 \n",
    "    Batch_Size = 50\n",
    "    Uni_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=Num_Epochs,verbose=1,validation_data=[ValInputs,ValTargets])\n",
    "\n",
    "    Dist_to_Threshold = acquisition_function_dist_to_threshold(Uni_model,TrainInputs,Num_Targets,1,10)\n",
    "\n",
    "    Easy_Dist_to_Threshold_ArgSort = np.flipud(Dist_to_Threshold.argsort())\n",
    "    Hard_Dist_to_Threshold_ArgSort = Dist_to_Threshold.argsort()\n",
    "\n",
    "    NumTasks = Num_Tasks\n",
    "    TaskSplitPoints = np.int(np.ceil(NumSamples/NumTasks))\n",
    "    print(TaskSplitPoints)\n",
    "    for i in range(NumTasks):\n",
    "        Num_Epochs_Task = Num_Epochs*(1/(i+1))\n",
    "\n",
    "        TaskInd = Hard_Dist_to_Threshold_ArgSort[0:np.min([(i+1)*TaskSplitPoints,NumSamples])]\n",
    "        D2THard_model.fit(TrainInputs[TaskInd,:],TrainTargets[TaskInd],batch_size=Batch_Size,epochs = np.int(Num_Epochs_Task))\n",
    "\n",
    "        TaskInd = Easy_Dist_to_Threshold_ArgSort[0:np.min([(i+1)*TaskSplitPoints,NumSamples])]\n",
    "        D2TEasy_model.fit(TrainInputs[TaskInd,:],TrainTargets[TaskInd],batch_size=Batch_Size,epochs = np.int(Num_Epochs_Task))\n",
    "\n",
    "    TestInputs = scipy.io.loadmat('MNIST_TestInputs.mat')\n",
    "    TestInputs = TestInputs['test_images']\n",
    "\n",
    "    TestTargets = scipy.io.loadmat('MNIST_TestTargets.mat')\n",
    "    TestTargets = TestTargets['test_targets']\n",
    "\n",
    "    TestError = np.zeros([3,2])\n",
    "    TestError[0,:] = D2THard_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[1,:] = D2TEasy_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[2,:] = Uni_model.test_on_batch(TestInputs,TestTargets)\n",
    "    \n",
    "    return TestError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:\n",
      "11\n",
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/100\n",
      "31050/45000 [===================>..........] - ETA: 2s - loss: 1.8271 - categorical_accuracy: 0.5020"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "NumTests = 100\n",
    "from IPython.display import clear_output\n",
    "\n",
    "TestError_Record = np.zeros([3,2,NumTests])\n",
    "\n",
    "for i in range(NumTests):\n",
    "    \n",
    "    clear_output()\n",
    "    print('Run:')\n",
    "    print(i)\n",
    "    TestError_Record[:,:,i] = MNIST_Bootstrap_Exp(5,100)\n",
    "    np.save('MNIST_Bootstrap_Perf_5',TestError_Record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
