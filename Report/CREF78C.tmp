\chapter{Introduction}

\section{Motivation}\label{sec:Intro_Motivation}

\textit{Supervised learning} is the area of machine learning in which algorithms learns the relationships between a set of input features and corresponding `ground truth' labels, the ultimate goal being to construct a predictive model of the relationships between the inputs and the labels in order to predict the labels of future, unseen input samples\cite{Witten2011}. Deep learning models perform this task by building hierarchical representations of the input features throughout a multitude of layers, often using feature maps such as convolutions to construct complex representations of the inputs \cite{lecun2015deep}. Supervised learning algorithms are fitted using a \textit{training set} of example input-label pairs; in deep models this is usually achieved by sampling uniformly from the entire training set throughout training, adjusting the model parameters to better fit the training data \cite{shamir2013stochastic}. \textit{Curriculum Learning} \cite{Bengio2009} \cite{louradour2014curriculum}, is the field of study which proposes that model learning performance can be improved by fitting the model on training samples in a meaningful order, typically by using 'easy' samples in the early stages of training in order to allow the model to learn basic concepts that be used as building blocks for training on more difficult samples \cite{ELMAN199371}. A similar area of research is \textit{active learning}, which is generally used when there is a prohibitive cost to obtaining labels for supervised learning \cite{settles2012active}. The goal of active learning is to identify which samples, usually from a pool of unlabeled candidate samples, would improve model performance the most if they were labeled and added to the training set \cite{settles2012active}. A common active learning approach is that of \textit{uncertainty sampling} \cite{settles2012active}, wherein the samples that the model is most uncertain about classifying are chosen to be included in the training data.

The substantial challenge with curriculum learning however is that in many domains however it is challenging to identify a clear delineation between `easy' and `hard' samples through which to implement curriculum learning \cite{Bengio2009} \cite{graves2017automated}. As interest in curriculum learning has grown, there have been an increasing number of studies into how this process can be curriculum construction can be automated so as to remove the requirement of often handcrafted curricula, for example \cite{graves2017automated} \cite{jiang2015self} \cite{pentina2015curriculum} \cite{weinshall2018curriculum}. In this paper we propose that methodologies developed for active learning approach, in particular uncertainty sampling, are well suited to estimating the difficulty of training samples, allowing for the automatic construction of learning curricula that will improve the training of deep networks on a wide range of machine learning problems. Specifically, we can use the classification uncertainty of deep models to approximate the difficulty of training samples, which can then be used to divide the training data up into tasks of different ascending difficulty, which can then be used to construct a learning curriculum without the need for a predefined notion of sample difficulty.

\section{Contribution}
In this paper we introduce two categories of automated curriculum construction, which we broadly term \textit{Active Curricula}. In Chapter \ref{ch:BootstrappedActiveCurricula} we implement the first approach, termed \textit{Bootstrapped Active Curricula}, wherein we use the classification uncertainty of a pre-trained baseline line model to estimate the difficulty of the training data in the Geometric Shapes \cite{GeoShapes} dataset. We use the uncertainty scores to divide the training data into equally sized tasks, training an identical curriculum model on the tasks in increasing order of difficulty. We show that the curriculum model consistently outperformed the baseline model, supporting the hypothesis that training with a curriculum can improve model performance, and suggesting that using model uncertainty can be an effective way of inferring sample difficulty in order to automate the curriculum construction process. In Chapter \ref{ch:DAC} we develop this idea further by introduced \textit{Dynamic Active Curricula}, where, instead of using a baseline model to estimate sample difficulty, the classification uncertainty of the curriculum model itself is calculated throughout training, resulting in a dynamic learning curriculum that adapts throughout the training process. We test several dynamic curriculum methods on the Geometric Shapes dataset, as well as the MNIST handwritten digit recognition task \ref{lecun-mnisthandwrittendigit-2010} and the CIFAR 10 image recognition dataset \ref{krizhevsky2009learning}. We show that 



\section{Document Structure}
The subsequent chapters of this thesis will be organised as follows:
\begin{itemize}
\item Background - Here we will introduce in more detail the topics introduced above, giving the reader the background required to understand and appreciate the rest of work. We will also develop some of the nomenclature that will be used in various other sections.
\item Related Work - In this chapter we set our contributions in the context of related studies, by discussing various other works which have tested approaches for automating curriculum discovery and improving learning performance with curriculum methods.
\item Bootstrapped Active Curricula - In this chapter we introduce the bootstrapped active curriculum approach, explaining the curriculum construction, the methods used to test it, as well as the results of the experiments and subsequent discussion.
\item Dynamic Active Curricula - Motivated by the previous chapter, we test methods for dynamically constructing and implementing learning curricula throughout training, again laying out the curriculum construction approach, the methods used to test its efficacy and a discussion of the results of the experiments.
\item Conclusion and Further Work - Finally, we summarise the findings of the thesis and suggest directions in which future work can build on the experiments shown in this paper.
\end{itemize}
In the next section we will introduce in more detail the topics introduced above, giving the reader the background required to understand and appreciate the rest of work. We will then set our contribution in the context of related studies that have also tested methods for automating curriculum discovery and improving learning performance with curriculum methods.The

In the next section we will introduce in more detail active and curriculum learning, exploring the link between the two approaches and the sometimes contradictory hypotheses they pose. We will then discuss related work where the authors implement similar methods for improving algorithm performance through biasing learning towards certain training samples throughout training. We will then lay out the experimental methodologies and datasets used in the paper before presenting and analysing the results of the tests and concluding with a discussion and suggestions for further work.