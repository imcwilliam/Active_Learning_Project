@book{Allgoer1980,
  author        = "E.L.Allgower and K.Georg",
  title         	= "Numerical continuation methods. An introduction",
  publisher   	= "Springer-Verlag",
  year        	= "1980",
}
@article{Bengio2009,
  author        = "Y.Bengio, J.Louradour, R.Collobert and J.Weston",
  title         = "Curriculum Learning",
  journal       = "Procedures of the International Conference on Machine Learning",
  volume        = 26,
  pages         = "41-48",
  year          = 2009,
}
@article{Bubeck2012,
  author        = "S.Bubeck, N.Cesa-Bianchi",
  title         	= "Regret Analysis of stochastic and nonstochastic multi-armed bandit problems",
  journal       = "Machine Learning",
  volume        = 5,
  pages         = "1-122",
  year          = 2012,
}
@article{Chang18,
  author        = "H-S Chang, E.Learned-Miller, A.McCallum",
  title         	= "Active Bias: Training More Accuracy Neural Networks by Emphasizing High Variance Samples",
  journal       = "Advances in Neural Information Processing Systems",
  volume        = 31,
  pages         = "1-122",
  year          = 2017,
}


%Deep Learning:
% Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. Nature, 521:436–, May 2015.

%Active Learning

%Curriculum Learning (or similar)
%''Training connectionist networks with queries and selective sampling''
%LEARNING TO EXECUTE -Wojciech Zaremba∗

%ADAM
%Adam: a Method for Stochastic Optimization

%Batch Normalization
%Sergey Ioffe and Christian Szegedy. Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift

%Gradient Descent
%	RUMELHART, D. E., HINTON, G. E., and WILLIAMS, R. J. (1986): Learning internal representations by error propagation. In Parallel distributed processing: Explorations in the microstructure of cognition, vol.I, 318-362, Bradford Books.

%SGD
%Large-scale machine learning with stochastic gradient descent, L Bottou - Proceedings of COMPSTAT'2010, 2010 - Springer
%Generalization Error
%Inference for the Generalization Error
