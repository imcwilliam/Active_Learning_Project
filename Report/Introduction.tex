\chapter{Introduction}

\section{Motivation}\label{sec:Intro_Motivation}
\subsection{Supverised Learning}

\subsection{Curriculum Learning}

\subsection{Active Learning}

\section{Contribution}

\section{Document Structure}
The subsequent chapters of this thesis will be organised as follows:
\begin{itemize}
\item Background - Here we will introduce in more detail the topics introduced above, giving the reader the background required to understand and appreciate the rest of work. We will also develop some of the nomenclature that will be used in various other sections.
\item Related Work - In this chaper we set our contributions in the context of related studies, by dicsussing various other works which have tested approaches for automating curriculum discovery and improving learning performance with curriculum methods.
\item Bootstrapped Active Curricula - In this chapter we introduce the bootstrapped active curriculum approach, explaining the curriculum construction, the methods used to test it, as well as the results of the experiments and subsequent discussion.
\item Dynamic Active Curricula - Motivated by the previous chapter, we test methods for dynamically constructing and implementing learning curricula throughout training, again laying out the curriculum construction approach, the methods used to test its efficacy and a discssion of the results of the experiments.
\item Conclusion and Further Work - Finally, we summarise the findings of the thesis and suggest directions in which future work can build on the experiments shown in this paper.
\end{itemize}
In the next section we will introduce in more detail the topics introduced above, giving the reader the background required to understand and appreciate the rest of work. We will then set our contribution in the context of related studies that have also tested methods for automating curriculum discovery and improving learning performance with curriculum methods.The


\textit{Supervised learning} is the area of machine learning in which algorithms learns the relationship between a set of input features and corresponding `ground truth' labels, the ultimate goal being to construct a predictive model of the relationships between the inputs and the labels in order to predict the labels of future, unseen input samples. Deep learning models perform this task by building hierarchical representations of the input features throughout a multitude of layers, often using feature maps such as convolutions or recurrent layers to construct complex representations of the inputs. When training a deep model a standard methodology is \textit{gradient descent}, which calculates the gradient of a chosen error function with respect to the free parameters of the model so as to tune the parameters in a way that will minise this error function on the training set of input-label pairs. A popular variant of the gradient descent algorithm is \textit{mini-batch stochastic gradient descent}, which uniformly samples mini-batches of a preset size from the available training data, performing a gradient descent update on each batch until the all training samples have been selected, then repeating until the network converges to a solution. Sampling uniformly from the training data ensures that the mini-batch gradient is an unbiased estimation of the gradient over the whole training set, however the estimation can exhibit high variance. In this paper we analyse approaches for augmenting mini-batch stochastic gradient descent (SGD), using methods inspired by two areas of study; \textit{active learning} and \textit{curriculum learning}. 

Active learning is generally used when there is a prohibitive cost to obtaining labels for supervised learning; in such cases it is desirable to know which samples will lead to the greatest best improvement in algorithm performance, selected from a set of unlabeled candidate samples. As such, there is a rich literature in active learning detailing how to choose the most informative samples, in particular using \textit{acquisition functions} to select which sample(s) to label and use for training. While active learning is usually employed to reduce labeling costs and speed up learning, curriculum learning explores the hypothesis that the overall accuracy of the network can be improved by presenting the training data to the algorithm in a meaningful order. Inspired by the way in which humans and animals learn, REF BENGIO suggest learning can be improved by emphasising easier concepts earlier on in training before introducing difficult samples, or by emphasising more difficult training samples later in training. In their paper REF BENGIO for example, the authors use a the `Geometric Shapes' dataset, consisting of images of geometric shapes of different complexities, to show that by initially training on `easy', regular shapes, test classification accuracy is improved. 

The substantial challenge with curriculum learning however is that in many domains however it is challenging to identify a clear delineation between `easy' and `hard' samples through which to implement curriculum learning; in this paper we propose that the methodologies developed for the active learning approach are well suited to estimating the difficulty of training samples, allowing the automatic construction of learning curricula that will improve the training of deep networks on a wide range of tasks. Specifically, the approach set out in this paper modifies the SGD algorithm by, instead of sampling with uniform probability, sampling training examples proportionally to some measure of `difficulty', as derived from an active learning style acquisition function metric. We test our methods on three image classification datasets; MNIST, CIFAR 10 and the GeoShapes dataset (a geometric shapes classification dataset with an established curriculum baseline), exploring a range of active learning metrics as well as several curriculum construction methods. Our results show consistent performance against a uniform sampling baseline, with significant reductions in test set error, robust to different network architectures, datasets and curriculum methodologies. The output of this work is a set of flexible methods for improving deep models in a wide range of tasks, as well as an investigation of how using the difficulty and uncertainty of training samples affect learning performance. 

In the next section we will introduce in more detail active and curriculum learning, exploring the link between the two approaches and the sometimes contradictory hypotheses they pose. We will then discuss related work where the authors implement similar methods for improving algorithm performance through biasing learning towards certain training samples throughout training. We will then lay out the experimental methodologies and datasets used in the paper before presenting and analysing the results of the tests and concluding with a discussion and suggestions for further work.