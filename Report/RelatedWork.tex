\chapter{Related Work}\label{Related Work}
In this chapter we discuss several works that have also implemented methods to automate the curriculum construction process, note that some of the content in this section is based on the author's `Informatics Project Proposal', completed in preparation for this thesis.
\section{Self-Paced Learning}
The first area we discuss is \textit{self-paced learning}, as defined in \cite{kumar2010self} and introduced briefly in Chapter \ref{ch:background}. In the original paper the authors reference the original curriculum learning paper by Bengio et al \cite{Bengio2009}, discussing curriculum learning and the difficulties posed by constructing hand crafted curricula and measuring training sample difficulty. Their solution is self-paced learning (SPL), an alternative which approaches sample difficulty in a different way. The authors note that ``in self-paced learning, the characterization of what is “easy” applies not to individual samples, but to sets of samples; a set of samples is easy if it admits a good fit in the model space''. To implement self-paced learning the authors introduce a regularization term into the objective function of a latent SSVM \cite{felzenszwalb2008discriminatively} model, using a vector indicating the difficulty of each training sample. The regularization vector is not constructed prior to training, rather it is fitted throughout training in conjunction with the latent variable model parameters, with the objective function being minimised when a suitably sparse selection of training samples with high likelihood are considered. The level of sparsity is controlled with a tuning parameter which is annealed throughout training until the entire training set is included in training. The authors of \cite{jiang2014self} build on the self-paced learning idea by adapting the method to prefer a diverse selection of samples, as opposed to simply samples with a high likelihood. The authors note that SPL can suffer from a lack of diversity in the training samples used during training as often the samples with high likelihood will be clustered together; by introducing an additional term encouraging diversity in the objective function, the authors correct for this effect and show an improvement on the usual SPL method. Finally, in \cite{jiang2015self}, the authors further develop the link between SPL and curriculum learning. In this paper the authors suggest that a disadvantage with SPL is the inability to introduce prior knowledge into into the learning process, unlike in curriculum learning where prior knowledge is often used to craft a curriculum. To implement their training method, which they term \textit{self-paced curriculum learning}, the authors combine the SPL and curriculum learning approaches, using a predefined curriculum to order the inclusion of training samples, then also including the usual SPL regularization term in the objective function. The authors demonstrate that their method outperforms SPL without using a prior curriculum and curriculum learning without SPL, testing their method on a multimedia event detection task.
\section{Transfer Learning}
In a recent paper, the authors of \cite{weinshall2018curriculum} implement an automated curriculum for image classification using transfer learning \cite{pan2010survey}. The authors use the CIFAR 100 dataset \cite{krizhevsky2009learning}, passing the images through a large pre-trained network (pre-trained on a different image classification task, hence the connection with transfer learning), then using the embeddings of the penultimate layer of the pre-trained network as inputs to a shallow classifier, in this case a support vector machine (SVM) \cite{hearst1998support}, which is then trained to classify the CIFAR 100 training images given the inputs from the pre-trained network. The distance to classification margin from the shallow classifier is then used as a measure of sample difficulty, sorting the samples according to the confidence of the SVM. This approach is similar to the method we lay out in Chapter \ref{ch:BootstrappedActiveCurricula}, however, instead of using a pre-trained network, we `bootstrap' a curriculum by training a baseline model on the target problem, then use the uncertainty of the baseline model to infer sample difficulty. The transfer learning approach removes the need to train a complex model, with the pre-trained network able to `transfer' its knowledge of image data to help infer the difficulty of the training samples. 
\section{Reinforcement Learning}
A different way of approaching the curriculum construction method is to take a `meta-learning' \cite{vilalta2002perspective} approach towards identifying the best order in which to include training samples throughout learning, with the best curriculum being a parameter that can be learned and optimised through a meta-learning approach. To this end, the authors of \cite{graves2017automated} lay out a method in which they use a reinforcement learning \cite{sutton1998introduction} approach to learn the best way to construct a curriculum for an n-gram language modeling problem \cite{kneser1995improved}. In the paper, the authors divide the data into several `tasks' of increasing complexity, then used a multi-armed bandit approach \cite{sutton1998introduction} to train a meta-learner to identify which order of tasks (i.e. a curriculum) should be used to train an LSTM model on the language modeling task. The `arms' or `actions' of the multi-armed bandit in this study therefore represent the choice to sample from the different tasks, with the reinforcement learning model being optimised to maximise learning performance of the LSTM model by sampling from the different tasks throughout training. Improvements in learning performance are measured in a variety of ways, but are broadly divided into ``loss-driven progress'', where the reinforcement algorithm is rewarded for reducing the loss of the LSTM algorithm after training on the samples selected by the reinforcement learning algorithm. Alternatively, ``Complexity-driven progress'' rewards the reinforcement algorithm for increasing the complexity of the LSTM algorithm; this is motivated on the concept of \textit{minimum description length} \cite{grunwaldminimum}, which suggests that the model complexity will ``increase most in response to the training examples from which the network is best able to generalise''. The metrics used to measure improvement in learning performance could be interesting from both a curriculum learning and an active learning perspective, offering novel ways to measure the potential benefit of training on a given sample. Interestingly, the authors show that, despite the fact they have not enforced any preference for beginning training with the easier tasks, the reinforcement learning algorithm autonomously learns to implement easy to hard learning curricula, with increasing task difficulty. 

\section{Active Bias}
Finally we discuss a recent paper titled ``Active Bias'' \cite{Chang18}, which inspired some of the sampling based methods used in the paper. While the authors do not implement a strict curriculum construction method, they investigate how varying the probability with which training examples are sampled throughout the training of deep models affect learning performance. In Chapter \ref{ch:DAC}, we will employ a similar approach, arguing that it can effectively act a form of stochastic curriculum, where instead of strictly limiting the samples used in different training phases, we instead bias the sampling probabilities towards easy/hard training examples. The authors of \cite{Chang18} test a variety of methods for biasing sampling probability throughout training, for example by recording how successful the model has been in classifying the training samples in prior epochs, and biasing the sampling probability either in favour of or against selecting the samples that the model has been most unsuccessful at classifying previously. This is similar to the dynamic sampling curriculum methods that we implement in Chapter \ref{ch:DAC}, although we use the model's prediction uncertainty as opposed to recording previous classification outcomes. The authors test their results over a variety of deep architectures and datasets, including image classification and text analysis, concluding that many of their sampling methods outperform default uniform sampling. As we will develop later in this paper, the authors discover a relationship between task difficulty and the most effective sampling method, with difficult learning problems being most improved by emphasizing easy training samples, and vice versa, with more easier classification tasks being most improved by biasing learning to focus on difficult training samples. These results are supported by those laid out in the results section of Chapter \ref{ch:DAC} of this paper, with more difficult image classification tasks seeing greater learning improvement when using easy to hard learning curricula than easier problems. 

The papers laid out in this section present a broad overview of the various methods that other studies have used to automate the process of constructing learning curricula, including papers that have inspired some of the techniques used in this thesis. The next two chapters will discuss the active curriculum methods we introduce and implement in this paper, outlining their construction methodology and the experiments carried out to test their effect on learning performance in deep models over various image classification tasks.


