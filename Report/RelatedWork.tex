\chapter{Related Work}\label{Related Work}
In this chapter we discuss several works that have also implemented methods to automate the curriculum construction process, note that some of the content in this section is based on the author's `Informatic Project Proposal', completed in preparation for this thesis.
\section{Self-Paced Learning}
The first area discuss is \textit{self-paced learning}, as introduced in \cite{kumar2010self}. In the original paper the authors reference \cite{Bengio2009}, discussing curriculum learning and the difficulties posed by constructing hand crafted curricula and measuring training sample difficulty. Their solution is self-paced learning (SPL), an alternative which approaches sample difficulty in a different way. The authors note that ``in self-paced learning, the characterization of what is “easy” applies not to individual samples, but to sets of samples; a set of samples is easy if it admits a good fit in the model space''. To implement self-paced learning the authors introduce a regularization term into the objective function of a latent SSVM \cite{felzenszwalb2008discriminatively} model, using a vector indicating the difficulty of each training sample. The regularization vector is not constructed prior to training, rather it is fitted throughout training in conjunction with the latent variable model parameters, with the objective function being minimised when a suitably sparse selection of training samples with high likelihood are considered. The level of sparsity is controlled with a tuning parameter which is annealed throughout training until the entire training set is included in training. The authors of \cite{jiang2014self} build on the self-paced learning idea by adapting the method to prefer a diverse selection of samples, as opposed to simply samples with a high likelihood. The authors note that SPL can suffer from a lack of diversity in the training samples used during training as often the samples with high likelihood will be clustered together,  by introducing an additional term encouraging diversity in the objective function, the authors correct for this effect and show an improvement on the usual SPL method. Finally, in \cite{jiang2015self}, the authors further develop the link between SPL and curriculum learning. In this paper the authors suggest that a disadvantage with SPL is the inability to introduce prior knowledge into into the learning process, unlike in curriculum learning where prior knowledge is often used to craft a curriculum. To implement their model, which they term \textit{self-paced curriculum learning}, the authors combine th SPL and curriculum learning approaches, using a predefined curriculum to order the inclusion of training samples, then also including the usual SPL regularization term in the objective function. The authors demonstrate that their method outperforms SPL without using a prior curriculum and curriculum learning without SPL, testing their method on a multimedia event detection task.
\section{Transfer Learning}
In a recent paper, the authors of \cite{weinshall2018curriculum} implement an automated curriculum for image classification using transfer learning \cite{pan2010survey}. The authors use the CIFAR 100 \cite{krizhevsky2009learning} dataset, passing the images through a large pre-trained network (pre-trained on a different image classification task, hence the connection with transfer learning), then using the embeddings of the penultimate layer of the pre-trained network as inputs to shallow classifier, in this case a support vector machine (SVM) \cite{hearst1998support}, which is then trained to classify the training images given the inputs from the pre-trained network. The distance to classification margin from the shallow classifier is then used as a measure of sample difficulty, sorting the samples according to the confidence of the SVM. This approach is similar to the method we lay out in Chapter \ref{ch:BootstrappedActiveCurricula}, however instead of using a pre-trained network and a shallow classifier we `bootstrap' a curriculum by training a baseline model, identical to the desired final architecture, on the target problem, then use the uncertainty of the baseline model to infer sample difficulty. The transfer learning approach removes the need to train a complex model from, with the pre-trained knowledge able to `transfer' its knowledge of image data to help infer the difficulty of the training samples. 

\section{Reinforcement Learning}

\section{Active Learning}