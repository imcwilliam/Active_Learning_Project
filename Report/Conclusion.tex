\chapter{Conclusion and Future Research Directions}
\section{Concluding Analysis}
In this thesis we have introduced the concepts of curriculum and active learning, and motivated the use of active learning uncertainty methods as a way of approximating sample uncertainty in order to automate the process of constructing learning curricula for deep models. In Chapter {ch:BootstrappedActiveCurricula} we implemented the first  curriculum construction approach, `Bootstrapped Active Curricula (BAC)'. In this approach, we estimated training sample uncertainty from a fully trained model, using the uncertainty scores to construct a task based curriculum and effectively `bootstrapping' a curriculum from a baseline model. We tested the method using the Geometric Shapes dataset, and the results laid out in Section \ref{sec:BAC_Results} show that the boostrapped active curriculum method consistently outperforms a baseline model trained uniformly on the entire training set, as well as the predefined curriculum approach used in \cite{Bengio2009}, a seminal paper in the curriculum learning literature. Analysing how the automated curriculum selects samples to use in the different training phases supports the use of uncertainty metrics as a measure of sample uncertainty, with the chosen samples corresponding well with an intuitive sense of difficulty in the data. An interesting result however was that the automated curriculum actually outperformed using a predefined curriculum, suggesting that inferring sample difficulty directly from the model being trained may lead to better results than using manually assigned difficulty ratings. We concluded this chapter by discussing some of the computational drawbacks of the boostrapped active curriculum approach, for example the need to first train a baseline model to implement the curriculum, before proposing the use of dynamic active curriculum methods.

In Chapter \ref{ch:DAC}, we implement several 'Dynamic Active Curricula (DAC)' methods; in contrast to the bootstrapped active curriculum approach sample difficulty is estimated throughout training using the curriculum model itself, as opposed to using a pre-trained baseline model. We tested three different variations of this method; the first, 'Dynamic Task Curricula', followed the BAC approach by repeatedly splitting the training set into separate tasks at the start of every epochs and training on increasingly difficult tasks throughout training. The 'Dynamic Sampled Curricula' method instead altered the usual mini-batch stochastic gradient descent optimisation algorithm by sampling from the training samples proportionally to the samples' relative classification uncertainty, as opposed to using a uniform sampling probability. Finally, the 'Dynamic Sampled Task Curricula' combined the two other dynamic curriculum approaches, diving the training set into tasks and then sampled mini-batches from the task proportionally to the uncertainty scores within the task. We again tested the different approaches on the Geometric Shapes dataset, as well as the MNIST handwritten digits recognition task and the CIFAR 10 image classification dataset. Again we found that most of the experiments consistently resulted in a higher test accuracy than a baseline model trained uniformly on the entire training set. As well as testing curricula where training progressed through increasingly difficult samples, as per the usual curriculum approach, we also tested the opposite, where training progresses through increasingly easy samples. Interestingly, in the dynamic curriculum methods we consistently found that using a 'hard to easy' curriculum led to superior results than the usual 'easy to hard' approach. We discussed why this may be the case, supporting ideas put forward in works such as \cite{Chang18} and \cite{weinshall2018curriculum} which suggest that the effect of curriculum methods may be affected by the difficulty of the learning task, relative to the capacity of the network being trained. We further analysed the experiments, investigating how the different uncertainty functions affected which samples are classified as easy or difficult, finding intuitive results in the case of the MNIST dataset, with the most uncertain samples corresponding to handwritten images that are indeed difficult to classify, even for a human observer. In the CIFAR 10 dataset the effect was less clear, particularly using the BALD \cite{houlsby2011bayesian} uncertainty function, however the curriculum nevertheless led to superior test performance than the baseline model, again suggesting that using the model to infer sample difficulty for curriculum construction may lead to even better results than using a handcrafted curriculum. 

Overall we feel the results of this study support both the hypothesis that curriculum learning can effectively be used to improve the generalization performance of deep models, and that using active learning approaches such as uncertainty estimation can be an effective way to construct such curricula in the absence of, and potentially even instead of, handcrafted curricula. The curriculum approaches laid out in this paper offer easily implementable methods to train deep models that can potentially be used with any network architecture, without significant computational cost. 

\section{Suggested Future Research Directions}
In this paper we tested various automated curriculum construction techniques, using active learning uncertainty methods to infer sample difficulty. There are several directions we would suggest for developing the work further; in the first instance we would broaden out some of the methods used in this paper, for example by testing different uncertainty functions or even other active learning methods such as query by committee or expected model change \cite{settles2012active}. Further work could also be done to analyse sensitivities to changing the parameters of the experiments, particularly those relevant to the curriculum construction, for example the chosen number of tasks. An open question as a result of this work is under what conditions do different curriculum methods best improve learning; we saw particularly in Chapter \ref{ch:DAC} that the easy to hard curricula performed best on the most difficult task, CIFAR 10. One could argue that, intuitively, a curriculum emphasising difficult samples may be best suited to easier tasks, in the same way that an expert in a field will learn most from studying tmore complex concepts, whereas a novice will benefit from beginning with learning more basic, foundational concepts.  Future experiments could test for this effect further with different datasets and architectures, potentially formalising the link between curriculum design and problem complexity. Another limitation with this work is that it had been constrained to image classification, future work could test these methods as applied to other machine learning tasks such as natural language processing, or, more broadly, regression tasks.



