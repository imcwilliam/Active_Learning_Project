{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_MNIST_Exp(Num_Epochs):\n",
    "    %matplotlib notebook\n",
    "    import keras as keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense,BatchNormalization,Dropout,Flatten, Conv1D, Conv2D,MaxPool2D\n",
    "    from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "    from keras.metrics import categorical_accuracy\n",
    "    from keras import regularizers,optimizers\n",
    "    from keras.regularizers import l2\n",
    "    import numpy as np\n",
    "    import scipy.io\n",
    "    import matplotlib.pyplot as plt\n",
    "    import gzip\n",
    "    from keras.layers.core import Lambda\n",
    "    from scipy.integrate import trapz\n",
    "    import seaborn as sns\n",
    "\n",
    "    from keras.layers.core import Lambda\n",
    "    from keras import backend as K    \n",
    "\n",
    "\n",
    "    def mini_batches(InputSample,BatchSize):\n",
    "        Index = np.array(range(InputSample.shape[0]),dtype=int)\n",
    "        NumBatches = np.int(InputSample.shape[0]/BatchSize)\n",
    "        Removed = np.array([],dtype=int)\n",
    "\n",
    "        BatchInd =[]\n",
    "        for BatchLoop in range(NumBatches):\n",
    "            RemainIndex = np.delete(Index,Removed)\n",
    "            SampleInd = np.random.choice(RemainIndex,size=BatchSize,replace=False)\n",
    "            Removed = np.append(Removed,SampleInd,axis=0)\n",
    "\n",
    "            BatchInd.append(SampleInd)\n",
    "        RemainIndex = np.delete(Index,Removed)\n",
    "        BatchInd.append(RemainIndex)\n",
    "\n",
    "        return BatchInd,NumBatches\n",
    "\n",
    "    def Get_Feats_and_Targets(filename):\n",
    "        import numpy as np\n",
    "\n",
    "        def line_to_Feats(line):\n",
    "            line = line.split(' ')\n",
    "            Feats = np.asarray(line[0:1024])\n",
    "            Target = np.zeros([3])\n",
    "            Target[int(line[1024])] = 1\n",
    "            return Feats,Target\n",
    "\n",
    "        f = open(filename, 'r')\n",
    "        lines = f.readlines()\n",
    "        Features = []\n",
    "        Targets = []\n",
    "        for i in range(len(lines)-1):\n",
    "            line = lines[i+1]\n",
    "            Feats,Tgts = line_to_Feats(line)\n",
    "            Features.append(Feats)\n",
    "            Targets.append(Tgts)\n",
    "\n",
    "        return np.asarray(Features,dtype = 'float64'), np.asarray(Targets,dtype = 'int')\n",
    "\n",
    "    \n",
    "    ConvFlag =1\n",
    "    \n",
    "    FullInputs = scipy.io.loadmat('MNIST_TrainInputs.mat')\n",
    "    FullInputs = FullInputs['images']\n",
    "\n",
    "    FullTargets = scipy.io.loadmat('MNIST_TrainTargets.mat')\n",
    "    FullTargets = FullTargets['targets']\n",
    "\n",
    "    Validation_Cutoff = 0.5\n",
    "\n",
    "    Validation_Cutoff = np.int(Validation_Cutoff*FullInputs.shape[0])\n",
    "\n",
    "    ValInputs = FullInputs[Validation_Cutoff:,:]\n",
    "    ValTargets = FullTargets[Validation_Cutoff:,:]\n",
    "\n",
    "    TrainInputs = FullInputs[0:Validation_Cutoff,:]\n",
    "    TrainTargets = FullTargets[0:Validation_Cutoff,:]\n",
    "    \n",
    "    if ConvFlag == 1:\n",
    "        TrainInputs = TrainInputs.reshape([TrainInputs.shape[0],28,28,1])\n",
    "        TrainInputs = np.swapaxes(TrainInputs,1,2)\n",
    "\n",
    "        ValInputs = ValInputs.reshape([ValInputs.shape[0],28,28,1])\n",
    "        ValInputs = np.swapaxes(ValInputs,1,2)\n",
    "        \n",
    "        data_dim2 = TrainInputs.shape[2]\n",
    "\n",
    "    data_dim = TrainInputs.shape[1]\n",
    "    NumSamples = TrainInputs.shape[0]\n",
    "    Num_Targets = TrainTargets.shape[-1]\n",
    "\n",
    "    index = np.linspace(0,NumSamples,NumSamples,endpoint=False,dtype=int)\n",
    "\n",
    "    reg_coeff = 0.001\n",
    "\n",
    "#     def Gen_Model(reg_coeff):\n",
    "#         model = Sequential()\n",
    "#         model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "#         model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "#         model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "#         model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "#         optim = optimizers.sgd(lr=0.001)\n",
    "#         model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "#         return model\n",
    "    \n",
    "    \n",
    "    def Gen_Model(reg_coeff):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(100,5,activation='relu',input_shape =(data_dim,data_dim2,1)))\n",
    "        model.add(MaxPool2D())\n",
    "        model.add(Conv2D(100,5,activation='relu'))\n",
    "        model.add(MaxPool2D())\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(100,activation='relu',kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "        optim = optimizers.sgd(lr=0.001)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    ISHard_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    ISEasy_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    Uni_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    ISHard_model.set_weights(Uni_model.get_weights())\n",
    "    ISEasy_model.set_weights(Uni_model.get_weights())\n",
    "\n",
    "    def average_dist_to_threshold(model,samples,Num_Targets):\n",
    "        Output = model.predict(samples)\n",
    "        Output -= 1/float(Num_Targets)\n",
    "        Dist_to_Threshold = np.mean(np.abs(Output),1)\n",
    "        return Dist_to_Threshold\n",
    "\n",
    "    def acquisition_function_dist_to_threshold(model,samples,Num_Targets,temperature=1,Rescale=0):\n",
    "        Output = model.predict(samples)\n",
    "        Output -= 1/float(Num_Targets)\n",
    "        Dist_to_Threshold = np.sum(np.abs(Output),1)\n",
    "        Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "        Exp_Dist_to_Threshold *= 1\n",
    "    #     Exp_Dist_to_Threshold = Dist_to_Threshold\n",
    "        Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "        if Rescale == 1:\n",
    "            Median_Prob = np.median(Sampling_Prob)\n",
    "            Min_Prob = np.min(Sampling_Prob)\n",
    "            Sampling_Prob[Sampling_Prob > Median_Prob] += Min_Prob/2\n",
    "            Sampling_Prob[Sampling_Prob < Median_Prob] -= Min_Prob/2\n",
    "        return Sampling_Prob\n",
    "\n",
    "    count = 0 \n",
    "    Num_Epochs = Num_Epochs\n",
    "    Num_BurnIn = 1\n",
    "    Batch_Size = 60\n",
    "\n",
    "    Smoothing_Constant = 0\n",
    "\n",
    "    Val_Error = np.zeros([Num_Epochs,3])\n",
    "    Val_Acc = np.zeros([Num_Epochs,3])\n",
    "\n",
    "    def Biased_Batch(Inputs,Targets,Batch_Size,Sampling_Probability,Weighting_Flag = 1):\n",
    "        while True: \n",
    "            index = np.linspace(0,Inputs.shape[0],Inputs.shape[0],endpoint=False,dtype=int)\n",
    "    #         Sampling_Probability = Sampling_Probability/Sampling_Probability.sum()\n",
    "            Batch = np.random.choice(index,size=Batch_Size,replace=False,p=Sampling_Probability)\n",
    "            Batch_Inputs = Inputs[Batch,:,:]\n",
    "            Batch_Targets = Targets[Batch,:]\n",
    "            if Weighting_Flag == 1:\n",
    "                FullWeights = Sampling_Probability**-1\n",
    "                FullWeights = FullWeights/np.mean(FullWeights)\n",
    "            elif Weighting_Flag == -1:\n",
    "                FullWeights = Sampling_Probability/np.mean(Sampling_Probability)\n",
    "            else:\n",
    "                FullWeights = np.ones(Inputs.shape[0])\n",
    "\n",
    "            Weights = FullWeights[Batch]\n",
    "            yield (Batch_Inputs,Batch_Targets,Weights)\n",
    "\n",
    "\n",
    "    for EpochLoop in  range(Num_Epochs):\n",
    "        if count < Num_BurnIn:\n",
    "    #         ISHard_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "    #         Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "    #         Val_Error[count,0] = Error[0]\n",
    "    #         Val_Acc[count,0] = Error[1]\n",
    "\n",
    "    #         ISEasy_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,verbose=0)\n",
    "    #         Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "    #         Val_Error[count,1] = Error[0]\n",
    "    #         Val_Acc[count,1] = Error[1]\n",
    "\n",
    "            Uni_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,verbose=0)\n",
    "            Error = Uni_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,2] = Error[0]\n",
    "            Val_Acc[count,2] = Error[1]\n",
    "\n",
    "            ISHard_model.set_weights(Uni_model.get_weights())\n",
    "            ISEasy_model.set_weights(Uni_model.get_weights())\n",
    "\n",
    "            Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,0] = Error[0]\n",
    "            Val_Acc[count,0] = Error[1]\n",
    "\n",
    "            Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,1] = Error[0]\n",
    "            Val_Acc[count,1] = Error[1]\n",
    "\n",
    "\n",
    "        else:\n",
    "            Hard_Sampling_Probability = acquisition_function_dist_to_threshold(ISHard_model,TrainInputs,Num_Targets,0.6)\n",
    "            Easy_SamplingProbability = 1/acquisition_function_dist_to_threshold(ISEasy_model,TrainInputs,Num_Targets,0.6)\n",
    "            Easy_SamplingProbability/= Easy_SamplingProbability.sum().astype(float)\n",
    "#             print(Hard_Sampling_Probability.max()/Hard_Sampling_Probability.min())\n",
    "\n",
    "            MaxProbInd = Hard_Sampling_Probability.argmax()\n",
    "            MinProbInd = Hard_Sampling_Probability.argmin()\n",
    "\n",
    "            ISHard_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,Hard_Sampling_Probability,0),\n",
    "                                       steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "            ISEasy_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,Easy_SamplingProbability,0),\n",
    "                                     steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "\n",
    "            Uni_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,1/np.float(NumSamples)*np.ones(NumSamples)),\n",
    "                                    steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "            Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,0] = Error[0]\n",
    "            Val_Acc[count,0] = Error[1]\n",
    "\n",
    "            Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,1] = Error[0]\n",
    "            Val_Acc[count,1] = Error[1]\n",
    "\n",
    "            Error = Uni_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,2] = Error[0]\n",
    "            Val_Acc[count,2] = Error[1]\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    TestInputs = scipy.io.loadmat('MNIST_TestInputs.mat')\n",
    "    TestInputs = TestInputs['test_images']\n",
    "    TestTargets = scipy.io.loadmat('MNIST_TestTargets.mat')\n",
    "    TestTargets = TestTargets['test_targets']\n",
    "\n",
    "    TestError = np.zeros([3,2])\n",
    "    TestError[0,:] = ISHard_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[1,:] = ISEasy_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[2,:] = Uni_model.test_on_batch(TestInputs,TestTargets)\n",
    "\n",
    "    return TestError, Val_Error,Val_Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "NumRuns = 1000\n",
    "NumEpochs = 256\n",
    "\n",
    "TestError = np.zeros([3,2,NumRuns])\n",
    "Val_Error = np.zeros([NumEpochs,3,NumRuns])\n",
    "Val_Acc = np.zeros([NumEpochs,3,NumRuns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (10000, 784)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-19bf12db7277>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Val_Error_MNIST_Conv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mVal_Error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Val_Acc_MNIST_Conv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mVal_Acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mTestError\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mVal_Error\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mVal_Acc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRun_MNIST_Exp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNumEpochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-facd7b9c337e>\u001b[0m in \u001b[0;36mRun_MNIST_Exp\u001b[1;34m(Num_Epochs)\u001b[0m\n\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m     \u001b[0mTestError\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 249\u001b[1;33m     \u001b[0mTestError\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mISHard_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTestInputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTestTargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    250\u001b[0m     \u001b[0mTestError\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mISEasy_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTestInputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTestTargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[0mTestError\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUni_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTestInputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mTestTargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ian mcwilliam\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[0;32m   1253\u001b[0m         x, y, sample_weights = self._standardize_user_data(\n\u001b[0;32m   1254\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1255\u001b[1;33m             sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1256\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1257\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ian mcwilliam\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    752\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             exception_prefix='input')\n\u001b[0m\u001b[0;32m    755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ian mcwilliam\\AppData\\Local\\Continuum\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    124\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 126\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    127\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_1_input to have 4 dimensions, but got array with shape (10000, 784)"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(NumRuns):\n",
    "    np.save(\"Test_Performance_MNIST_Conv\",TestError)\n",
    "    np.save(\"Val_Error_MNIST_Conv\",Val_Error)\n",
    "    np.save(\"Val_Acc_MNIST_Conv\",Val_Acc)\n",
    "    TestError[:,:,i],Val_Error[:,:,i],Val_Acc[:,:,i] = Run_MNIST_Exp(NumEpochs)\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.78661132, 1.37352991])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Val_Error[:,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Val_Error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
