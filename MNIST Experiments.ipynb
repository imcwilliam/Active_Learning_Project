{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Run_MNIST_Exp(Num_Epochs):\n",
    "    %matplotlib notebook\n",
    "    import keras as keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import LSTM, Dense,BatchNormalization,Dropout,Flatten, Conv1D, Conv2D,MaxPool2D\n",
    "    from keras.losses import binary_crossentropy, categorical_crossentropy\n",
    "    from keras.metrics import categorical_accuracy\n",
    "    from keras import regularizers,optimizers\n",
    "    from keras.regularizers import l2\n",
    "    import numpy as np\n",
    "    import scipy.io\n",
    "    import matplotlib.pyplot as plt\n",
    "    import gzip\n",
    "    from keras.layers.core import Lambda\n",
    "    from scipy.integrate import trapz\n",
    "    import seaborn as sns\n",
    "\n",
    "    from keras.layers.core import Lambda\n",
    "    from keras import backend as K    \n",
    "\n",
    "\n",
    "    def mini_batches(InputSample,BatchSize):\n",
    "        Index = np.array(range(InputSample.shape[0]),dtype=int)\n",
    "        NumBatches = np.int(InputSample.shape[0]/BatchSize)\n",
    "        Removed = np.array([],dtype=int)\n",
    "\n",
    "        BatchInd =[]\n",
    "        for BatchLoop in range(NumBatches):\n",
    "            RemainIndex = np.delete(Index,Removed)\n",
    "            SampleInd = np.random.choice(RemainIndex,size=BatchSize,replace=False)\n",
    "            Removed = np.append(Removed,SampleInd,axis=0)\n",
    "\n",
    "            BatchInd.append(SampleInd)\n",
    "        RemainIndex = np.delete(Index,Removed)\n",
    "        BatchInd.append(RemainIndex)\n",
    "\n",
    "        return BatchInd,NumBatches\n",
    "\n",
    "    def Get_Feats_and_Targets(filename):\n",
    "        import numpy as np\n",
    "\n",
    "        def line_to_Feats(line):\n",
    "            line = line.split(' ')\n",
    "            Feats = np.asarray(line[0:1024])\n",
    "            Target = np.zeros([3])\n",
    "            Target[int(line[1024])] = 1\n",
    "            return Feats,Target\n",
    "\n",
    "        f = open(filename, 'r')\n",
    "        lines = f.readlines()\n",
    "        Features = []\n",
    "        Targets = []\n",
    "        for i in range(len(lines)-1):\n",
    "            line = lines[i+1]\n",
    "            Feats,Tgts = line_to_Feats(line)\n",
    "            Features.append(Feats)\n",
    "            Targets.append(Tgts)\n",
    "\n",
    "        return np.asarray(Features,dtype = 'float64'), np.asarray(Targets,dtype = 'int')\n",
    "\n",
    "    \n",
    "    ConvFlag = 1\n",
    "    \n",
    "    FullInputs = scipy.io.loadmat('MNIST_TrainInputs.mat')\n",
    "    FullInputs = FullInputs['images']\n",
    "\n",
    "    FullTargets = scipy.io.loadmat('MNIST_TrainTargets.mat')\n",
    "    FullTargets = FullTargets['targets']\n",
    "\n",
    "    Validation_Cutoff = 0.5\n",
    "\n",
    "    Validation_Cutoff = np.int(Validation_Cutoff*FullInputs.shape[0])\n",
    "\n",
    "    ValInputs = FullInputs[Validation_Cutoff:,:]\n",
    "    ValTargets = FullTargets[Validation_Cutoff:,:]\n",
    "\n",
    "    TrainInputs = FullInputs[0:Validation_Cutoff,:]\n",
    "    TrainTargets = FullTargets[0:Validation_Cutoff,:]\n",
    "    \n",
    "    if ConvFlag == 1:\n",
    "        TrainInputs = TrainInputs.reshape([TrainInputs.shape[0],28,28,1])\n",
    "        TrainInputs = np.swapaxes(TrainInputs,1,2)\n",
    "\n",
    "        ValInputs = ValInputs.reshape([ValInputs.shape[0],28,28,1])\n",
    "        ValInputs = np.swapaxes(ValInputs,1,2)\n",
    "        \n",
    "        data_dim2 = TrainInputs.shape[2]\n",
    "\n",
    "    data_dim = TrainInputs.shape[1]\n",
    "    NumSamples = TrainInputs.shape[0]\n",
    "    Num_Targets = TrainTargets.shape[-1]\n",
    "\n",
    "    index = np.linspace(0,NumSamples,NumSamples,endpoint=False,dtype=int)\n",
    "\n",
    "    reg_coeff = 0.001\n",
    "\n",
    "#     def Gen_Model(reg_coeff):\n",
    "#         model = Sequential()\n",
    "#         model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "#         model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "#         model.add(Dense(100,activation='tanh',input_shape =(data_dim,),kernel_regularizer=l2(reg_coeff)))\n",
    "#         model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "#         optim = optimizers.sgd(lr=0.001)\n",
    "#         model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "#         return model\n",
    "    \n",
    "    \n",
    "    def Gen_Model(reg_coeff):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(100,5,activation='relu',input_shape =(data_dim,data_dim2,1)))\n",
    "        model.add(MaxPool2D())\n",
    "        model.add(Conv2D(100,5,activation='relu'))\n",
    "        model.add(MaxPool2D())\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(100,activation='relu',kernel_regularizer=l2(reg_coeff)))\n",
    "        model.add(Dense(Num_Targets,activation = 'softmax',kernel_regularizer=l2(reg_coeff),input_shape =(data_dim,)))\n",
    "        optim = optimizers.sgd(lr=0.001)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=optim,metrics=['categorical_accuracy'])\n",
    "\n",
    "        return model\n",
    "\n",
    "    ISHard_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    ISEasy_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    Uni_model = Gen_Model(reg_coeff)\n",
    "\n",
    "    ISHard_model.set_weights(Uni_model.get_weights())\n",
    "    ISEasy_model.set_weights(Uni_model.get_weights())\n",
    "\n",
    "    def average_dist_to_threshold(model,samples,Num_Targets):\n",
    "        Output = model.predict(samples)\n",
    "        Output -= 1/float(Num_Targets)\n",
    "        Dist_to_Threshold = np.mean(np.abs(Output),1)\n",
    "        return Dist_to_Threshold\n",
    "\n",
    "    def acquisition_function_dist_to_threshold(model,samples,Num_Targets,temperature=1,Rescale=0):\n",
    "        Output = model.predict(samples)\n",
    "        Output -= 1/float(Num_Targets)\n",
    "        Dist_to_Threshold = np.sum(np.abs(Output),1)\n",
    "        Exp_Dist_to_Threshold = np.exp(Dist_to_Threshold/temperature)\n",
    "        Exp_Dist_to_Threshold *= 1\n",
    "    #     Exp_Dist_to_Threshold = Dist_to_Threshold\n",
    "        Sampling_Prob = Exp_Dist_to_Threshold/np.sum(Exp_Dist_to_Threshold).astype(float)\n",
    "        if Rescale == 1:\n",
    "            Median_Prob = np.median(Sampling_Prob)\n",
    "            Min_Prob = np.min(Sampling_Prob)\n",
    "            Sampling_Prob[Sampling_Prob > Median_Prob] += Min_Prob/2\n",
    "            Sampling_Prob[Sampling_Prob < Median_Prob] -= Min_Prob/2\n",
    "        return Sampling_Prob\n",
    "\n",
    "    count = 0 \n",
    "    Num_Epochs = Num_Epochs\n",
    "    Num_BurnIn = 1\n",
    "    Batch_Size = 100\n",
    "\n",
    "    Smoothing_Constant = 0\n",
    "\n",
    "    Val_Error = np.zeros([Num_Epochs,3])\n",
    "    Val_Acc = np.zeros([Num_Epochs,3])\n",
    "\n",
    "    def Biased_Batch(Inputs,Targets,Batch_Size,Sampling_Probability,Weighting_Flag = 1):\n",
    "        while True: \n",
    "            index = np.linspace(0,Inputs.shape[0],Inputs.shape[0],endpoint=False,dtype=int)\n",
    "    #         Sampling_Probability = Sampling_Probability/Sampling_Probability.sum()\n",
    "            Batch = np.random.choice(index,size=Batch_Size,replace=False,p=Sampling_Probability)\n",
    "            Batch_Inputs = Inputs[Batch,:,:]\n",
    "            Batch_Targets = Targets[Batch,:]\n",
    "            if Weighting_Flag == 1:\n",
    "                FullWeights = Sampling_Probability**-1\n",
    "                FullWeights = FullWeights/np.mean(FullWeights)\n",
    "            elif Weighting_Flag == -1:\n",
    "                FullWeights = Sampling_Probability/np.mean(Sampling_Probability)\n",
    "            else:\n",
    "                FullWeights = np.ones(Inputs.shape[0])\n",
    "\n",
    "            Weights = FullWeights[Batch]\n",
    "            yield (Batch_Inputs,Batch_Targets,Weights)\n",
    "\n",
    "\n",
    "    for EpochLoop in  range(Num_Epochs):\n",
    "        if count < Num_BurnIn:\n",
    "    #         ISHard_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,validation_data=[ValInputs,ValTargets],verbose=0)\n",
    "    #         Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "    #         Val_Error[count,0] = Error[0]\n",
    "    #         Val_Acc[count,0] = Error[1]\n",
    "\n",
    "    #         ISEasy_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,verbose=0)\n",
    "    #         Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "    #         Val_Error[count,1] = Error[0]\n",
    "    #         Val_Acc[count,1] = Error[1]\n",
    "\n",
    "            Uni_model.fit(TrainInputs,TrainTargets,batch_size = Batch_Size,epochs=1,verbose=1)\n",
    "            Error = Uni_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,2] = Error[0]\n",
    "            Val_Acc[count,2] = Error[1]\n",
    "\n",
    "            ISHard_model.set_weights(Uni_model.get_weights())\n",
    "            ISEasy_model.set_weights(Uni_model.get_weights())\n",
    "\n",
    "            Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,0] = Error[0]\n",
    "            Val_Acc[count,0] = Error[1]\n",
    "\n",
    "            Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,1] = Error[0]\n",
    "            Val_Acc[count,1] = Error[1]\n",
    "\n",
    "\n",
    "        else:\n",
    "            Hard_Sampling_Probability = acquisition_function_dist_to_threshold(ISHard_model,TrainInputs,Num_Targets,0.6)\n",
    "            Easy_SamplingProbability = 1/acquisition_function_dist_to_threshold(ISEasy_model,TrainInputs,Num_Targets,0.6)\n",
    "            Easy_SamplingProbability/= Easy_SamplingProbability.sum().astype(float)\n",
    "#             print(Hard_Sampling_Probability.max()/Hard_Sampling_Probability.min())\n",
    "\n",
    "            MaxProbInd = Hard_Sampling_Probability.argmax()\n",
    "            MinProbInd = Hard_Sampling_Probability.argmin()\n",
    "\n",
    "            ISHard_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,Hard_Sampling_Probability,0),\n",
    "                                       steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "            ISEasy_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,Easy_SamplingProbability,0),\n",
    "                                     steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "\n",
    "            Uni_model.fit_generator(Biased_Batch(TrainInputs,TrainTargets,Batch_Size,1/np.float(NumSamples)*np.ones(NumSamples)),\n",
    "                                    steps_per_epoch=np.int(NumSamples/Batch_Size),verbose=0)\n",
    "\n",
    "            Error = ISHard_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,0] = Error[0]\n",
    "            Val_Acc[count,0] = Error[1]\n",
    "\n",
    "            Error = ISEasy_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,1] = Error[0]\n",
    "            Val_Acc[count,1] = Error[1]\n",
    "\n",
    "            Error = Uni_model.test_on_batch(ValInputs,ValTargets)\n",
    "            Val_Error[count,2] = Error[0]\n",
    "            Val_Acc[count,2] = Error[1]\n",
    "            \n",
    "            print(EpochLoop)\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    TestInputs = scipy.io.loadmat('MNIST_TestInputs.mat')\n",
    "    TestInputs = TestInputs['test_images']\n",
    "    TestTargets = scipy.io.loadmat('MNIST_TestTargets.mat')\n",
    "    TestTargets = TestTargets['test_targets']\n",
    "    \n",
    "    TestInputs = TestInputs.reshape([TestInputs.shape[0],28,28,1])\n",
    "    TestInputs = np.swapaxes(TestInputs,1,2)\n",
    "\n",
    "    TestError = np.zeros([3,2])\n",
    "    TestError[0,:] = ISHard_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[1,:] = ISEasy_model.test_on_batch(TestInputs,TestTargets)\n",
    "    TestError[2,:] = Uni_model.test_on_batch(TestInputs,TestTargets)\n",
    "    \n",
    "    \n",
    "\n",
    "    return TestError, Val_Error,Val_Acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import datetime\n",
    "NumRuns = 1\n",
    "NumEpochs = 50\n",
    "\n",
    "TestError = np.zeros([3,2,NumRuns])\n",
    "Val_Error = np.zeros([NumEpochs,3,NumRuns])\n",
    "Val_Acc = np.zeros([NumEpochs,3,NumRuns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "30000/30000 [==============================] - 78s 3ms/step - loss: 2.4863 - categorical_accuracy: 0.1368\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(NumRuns):\n",
    "    np.save(\"Test_Performance_MNIST_Conv\",TestError)\n",
    "    np.save(\"Val_Error_MNIST_Conv\",Val_Error)\n",
    "    np.save(\"Val_Acc_MNIST_Conv\",Val_Acc)\n",
    "    TestError[:,:,i],Val_Error[:,:,i],Val_Acc[:,:,i] = Run_MNIST_Exp(NumEpochs)\n",
    "    print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save(\"Test_Performance_MNIST_Conv\",TestError)\n",
    "np.save(\"Val_Error_MNIST_Conv\",Val_Error)\n",
    "np.save(\"Val_Acc_MNIST_Conv\",Val_Acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Val_Error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
